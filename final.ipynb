{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Required Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "import torch\n",
    "from torchtext import data\n",
    "import spacy\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas(desc='Progress')\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from torchtext.data import Example\n",
    "from sklearn.metrics import f1_score\n",
    "import torchtext\n",
    "import os \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# cross validation and metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1024):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (56370, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "print(\"Train shape : \",train.shape)\n",
    "print(\"Test shape : \",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1362492, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = pd.concat([train, test])\n",
    "total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose =  True):\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1362492/1362492 [00:03<00:00, 359908.55it/s]\n",
      "100%|██████████| 1362492/1362492 [00:03<00:00, 442383.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'How': 273144, 'did': 34918, 'Quebec': 102, 'nationalists': 97, 'see': 9397}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = total[\"question_text\"].progress_apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)\n",
    "print({k: vocab[k] for k in list(vocab)[:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 120000\n",
    "max_len = 72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOOGLE NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "news_path = 'GoogleNews-vectors-negative300.bin'\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522569/522569 [00:00<00:00, 547433.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 24.05% of vocab\n",
      "Found embeddings for  78.75% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 420476),\n",
       " ('a', 419837),\n",
       " ('of', 345145),\n",
       " ('and', 262815),\n",
       " ('India?', 17082),\n",
       " ('it?', 13436),\n",
       " ('do?', 9112),\n",
       " ('life?', 8074),\n",
       " ('you?', 6553),\n",
       " ('me?', 6485)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'?' in embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'&' in embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = 'glove.840B.300d.txt'\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(new_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522569/522569 [00:00<00:00, 942041.56it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 32.91% of vocab\n",
      "Found embeddings for  88.16% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('India?', 17082),\n",
       " ('it?', 13436),\n",
       " (\"What's\", 12985),\n",
       " ('do?', 9112),\n",
       " ('life?', 8074),\n",
       " ('you?', 6553),\n",
       " ('me?', 6485),\n",
       " ('them?', 6421),\n",
       " ('time?', 5994),\n",
       " ('world?', 5632)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'?' in embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'&' in embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAST TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = 'wiki-news-300d-1M.vec'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(new_path) if len(o)>100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522569/522569 [00:00<00:00, 975716.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 29.77% of vocab\n",
      "Found embeddings for  87.66% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('India?', 17082),\n",
       " (\"don't\", 15642),\n",
       " ('it?', 13436),\n",
       " (\"I'm\", 13344),\n",
       " (\"What's\", 12985),\n",
       " ('do?', 9112),\n",
       " ('life?', 8074),\n",
       " (\"can't\", 7375),\n",
       " ('you?', 6553),\n",
       " ('me?', 6485)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'?' in embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'&' in embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = 'paragram_300_sl999.txt'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(new_path, encoding=\"utf8\", errors='ignore') if len(o)>100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522569/522569 [00:00<00:00, 977781.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 19.42% of vocab\n",
      "Found embeddings for  72.21% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What', 436013),\n",
       " ('I', 319441),\n",
       " ('How', 273144),\n",
       " ('Why', 148582),\n",
       " ('Is', 113627),\n",
       " ('Can', 54992),\n",
       " ('Which', 49357),\n",
       " ('Do', 41756),\n",
       " ('If', 35896),\n",
       " ('Are', 30442)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1362492/1362492 [00:07<00:00, 176494.47it/s]\n",
      "100%|██████████| 1362492/1362492 [00:02<00:00, 503774.33it/s]\n"
     ]
    }
   ],
   "source": [
    "total[\"question_text\"] = total[\"question_text\"].progress_apply(lambda x: clean_text(x))\n",
    "sentences = total[\"question_text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = 'glove.840B.300d.txt'\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(new_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 259819/259819 [00:00<00:00, 943118.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 70.96% of vocab\n",
      "Found embeddings for  99.44% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Quorans', 885),\n",
       " ('Brexit', 510),\n",
       " ('cryptocurrencies', 506),\n",
       " ('Redmi', 394),\n",
       " ('OnePlus', 130),\n",
       " ('UCEED', 126),\n",
       " ('GDPR', 110),\n",
       " ('Blockchain', 110),\n",
       " ('demonetisation', 109),\n",
       " ('Coinbase', 105),\n",
       " ('BNBR', 104),\n",
       " ('Machedo', 103),\n",
       " ('Adityanath', 101),\n",
       " ('Boruto', 96),\n",
       " ('ethereum', 94),\n",
       " ('DCEU', 93),\n",
       " ('IIEST', 90),\n",
       " ('SJWs', 86),\n",
       " ('Qoura', 81),\n",
       " ('LNMIIT', 72)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mispell_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1362492/1362492 [00:25<00:00, 53877.12it/s]\n",
      "Progress: 100%|██████████| 1362492/1362492 [00:03<00:00, 413260.01it/s]\n",
      "100%|██████████| 1362492/1362492 [00:02<00:00, 486661.15it/s]\n"
     ]
    }
   ],
   "source": [
    "total[\"question_text\"] = total[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "sentences = total[\"question_text\"].progress_apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 259726/259726 [00:00<00:00, 974089.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 70.96% of vocab\n",
      "Found embeddings for  99.44% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def add_features(df):\n",
    "    \n",
    "    df['question_text'] = df['question_text'].progress_apply(lambda x:str(x))\n",
    "    df['total_length'] = df['question_text'].progress_apply(len)\n",
    "    df['capitals'] = df['question_text'].progress_apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.progress_apply(lambda row: float(row['capitals'])/(float(row['total_length']) + 0.0000001),\n",
    "                                axis=1)\n",
    "    df['num_words'] = df.question_text.str.count('\\S+')\n",
    "    df['num_unique_words'] = df['question_text'].progress_apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / (df['num_words'] + 0.0000001)\n",
    "    return df\n",
    "\n",
    "def load_and_prec():\n",
    "    \n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    print(\"Train shape : \",train_df.shape)\n",
    "    print(\"Test shape : \",test_df.shape)\n",
    "    \n",
    "    # lower\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "\n",
    "    # Clean the text\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_text(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "    \n",
    "    # Clean speelings\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "    \n",
    "    ## fill up the missing values\n",
    "    train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n",
    "    test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n",
    "    \n",
    "    train = add_features(train_df)\n",
    "    test = add_features(test_df)\n",
    "\n",
    "    features = train[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "    test_features = test[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(np.vstack((features, test_features)))\n",
    "    features = ss.transform(features)\n",
    "    test_features = ss.transform(test_features)\n",
    "\n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words = max_features)\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=max_len)\n",
    "    test_X = pad_sequences(test_X, maxlen=max_len)\n",
    "\n",
    "    ## Get the target values\n",
    "    train_y = train_df['target'].values \n",
    "    \n",
    "    #shuffling the data\n",
    "    np.random.seed(1024)\n",
    "    trn_idx = np.random.permutation(len(train_X))\n",
    "\n",
    "    train_X = train_X[trn_idx]\n",
    "    train_y = train_y[trn_idx]\n",
    "    \n",
    "    return train_X, test_X, train_y, features, test_features, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (56370, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1306122/1306122 [00:07<00:00, 178536.75it/s]\n",
      "Progress: 100%|██████████| 1306122/1306122 [00:25<00:00, 51782.78it/s]\n",
      "Progress: 100%|██████████| 1306122/1306122 [00:00<00:00, 1329658.81it/s]\n",
      "Progress: 100%|██████████| 1306122/1306122 [00:00<00:00, 1381459.09it/s]\n",
      "Progress: 100%|██████████| 1306122/1306122 [00:05<00:00, 248070.98it/s]\n",
      "Progress: 100%|██████████| 1306122/1306122 [00:20<00:00, 65206.82it/s]\n",
      "Progress: 100%|██████████| 1306122/1306122 [00:03<00:00, 353062.34it/s]\n",
      "Progress: 100%|██████████| 56370/56370 [00:00<00:00, 1134994.22it/s]\n",
      "Progress: 100%|██████████| 56370/56370 [00:00<00:00, 1251312.09it/s]\n",
      "Progress: 100%|██████████| 56370/56370 [00:00<00:00, 237088.94it/s]\n",
      "Progress: 100%|██████████| 56370/56370 [00:00<00:00, 64995.40it/s]\n",
      "Progress: 100%|██████████| 56370/56370 [00:00<00:00, 343102.93it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, features, test_features, word_index = load_and_prec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"x_train\",x_train)\n",
    "np.save(\"x_test\",x_test)\n",
    "np.save(\"y_train\",y_train)\n",
    "np.save(\"word_index.npy\",word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfvec = TfidfVectorizer(lowercase=False, vocabulary=word_index.keys())\n",
    "tfidf_vec = tfidfvec.fit_transform(list(total.question_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'what': 2,\n",
       " 'is': 3,\n",
       " 'a': 4,\n",
       " 'to': 5,\n",
       " 'in': 6,\n",
       " 'of': 7,\n",
       " 'i': 8,\n",
       " 'how': 9,\n",
       " 'and': 10,\n",
       " 'do': 11,\n",
       " 'are': 12,\n",
       " 'for': 13,\n",
       " 'you': 14,\n",
       " 'can': 15,\n",
       " 'why': 16,\n",
       " 'it': 17,\n",
       " 'my': 18,\n",
       " 'that': 19,\n",
       " 'if': 20,\n",
       " 'with': 21,\n",
       " 'on': 22,\n",
       " 'or': 23,\n",
       " 'have': 24,\n",
       " 'be': 25,\n",
       " 'does': 26,\n",
       " 's': 27,\n",
       " 'from': 28,\n",
       " 'your': 29,\n",
       " 'an': 30,\n",
       " 'which': 31,\n",
       " 'should': 32,\n",
       " 'when': 33,\n",
       " 'get': 34,\n",
       " 'best': 35,\n",
       " 'would': 36,\n",
       " 'as': 37,\n",
       " 'people': 38,\n",
       " 'some': 39,\n",
       " 'there': 40,\n",
       " 'who': 41,\n",
       " 'will': 42,\n",
       " 'like': 43,\n",
       " 'at': 44,\n",
       " 'not': 45,\n",
       " 't': 46,\n",
       " 'about': 47,\n",
       " 'they': 48,\n",
       " 'by': 49,\n",
       " 'did': 50,\n",
       " 'was': 51,\n",
       " 'we': 52,\n",
       " 'any': 53,\n",
       " 'so': 54,\n",
       " 'good': 55,\n",
       " 'me': 56,\n",
       " 'their': 57,\n",
       " 'one': 58,\n",
       " 'india': 59,\n",
       " 'has': 60,\n",
       " 'after': 61,\n",
       " 'most': 62,\n",
       " 'where': 63,\n",
       " 'make': 64,\n",
       " 'this': 65,\n",
       " 'but': 66,\n",
       " 'more': 67,\n",
       " 'all': 68,\n",
       " 'think': 69,\n",
       " 'many': 70,\n",
       " 'between': 71,\n",
       " 'time': 72,\n",
       " 'than': 73,\n",
       " 'much': 74,\n",
       " 'other': 75,\n",
       " 'someone': 76,\n",
       " 'he': 77,\n",
       " 'life': 78,\n",
       " 'use': 79,\n",
       " 'out': 80,\n",
       " 'way': 81,\n",
       " 'am': 82,\n",
       " 'us': 83,\n",
       " 'know': 84,\n",
       " 'up': 85,\n",
       " 'being': 86,\n",
       " 'work': 87,\n",
       " 'want': 88,\n",
       " 'take': 89,\n",
       " 'them': 90,\n",
       " 'were': 91,\n",
       " 'ever': 92,\n",
       " 'world': 93,\n",
       " 'his': 94,\n",
       " 'find': 95,\n",
       " 'become': 96,\n",
       " 'without': 97,\n",
       " 'just': 98,\n",
       " 'person': 99,\n",
       " 'could': 100,\n",
       " 'don': 101,\n",
       " 'feel': 102,\n",
       " 'year': 103,\n",
       " 'into': 104,\n",
       " 'better': 105,\n",
       " 'quora': 106,\n",
       " 'no': 107,\n",
       " 'go': 108,\n",
       " 'new': 109,\n",
       " 'm': 110,\n",
       " 'trump': 111,\n",
       " 'possible': 112,\n",
       " 'job': 113,\n",
       " 'only': 114,\n",
       " 'her': 115,\n",
       " 'years': 116,\n",
       " 'been': 117,\n",
       " 'indian': 118,\n",
       " 'mean': 119,\n",
       " 'women': 120,\n",
       " 'used': 121,\n",
       " 'need': 122,\n",
       " 'start': 123,\n",
       " 'first': 124,\n",
       " 'had': 125,\n",
       " 'difference': 126,\n",
       " 'money': 127,\n",
       " 'still': 128,\n",
       " 'different': 129,\n",
       " 'school': 130,\n",
       " 'country': 131,\n",
       " 'really': 132,\n",
       " 'long': 133,\n",
       " 'while': 134,\n",
       " 'old': 135,\n",
       " 'over': 136,\n",
       " 'our': 137,\n",
       " 'learn': 138,\n",
       " 'business': 139,\n",
       " 'same': 140,\n",
       " 'now': 141,\n",
       " 'love': 142,\n",
       " 'its': 143,\n",
       " 'before': 144,\n",
       " 'college': 145,\n",
       " 'things': 146,\n",
       " '2': 147,\n",
       " 'she': 148,\n",
       " 'even': 149,\n",
       " 'engineering': 150,\n",
       " 'give': 151,\n",
       " 'help': 152,\n",
       " 'during': 153,\n",
       " 'online': 154,\n",
       " 'day': 155,\n",
       " 'say': 156,\n",
       " 'see': 157,\n",
       " 'men': 158,\n",
       " 'bad': 159,\n",
       " 'because': 160,\n",
       " 'using': 161,\n",
       " 'book': 162,\n",
       " 'university': 163,\n",
       " 'company': 164,\n",
       " 'back': 165,\n",
       " 'change': 166,\n",
       " 'high': 167,\n",
       " 'sex': 168,\n",
       " 'then': 169,\n",
       " 'him': 170,\n",
       " 'anyone': 171,\n",
       " 'made': 172,\n",
       " 'stop': 173,\n",
       " 'girl': 174,\n",
       " 'having': 175,\n",
       " 'getting': 176,\n",
       " 'right': 177,\n",
       " '2017': 178,\n",
       " 'student': 179,\n",
       " 'live': 180,\n",
       " 'buy': 181,\n",
       " '1': 182,\n",
       " 'two': 183,\n",
       " 'study': 184,\n",
       " 'own': 185,\n",
       " 'thing': 186,\n",
       " 'happen': 187,\n",
       " 'science': 188,\n",
       " 'going': 189,\n",
       " 'name': 190,\n",
       " 'countries': 191,\n",
       " 'white': 192,\n",
       " 'something': 193,\n",
       " 'china': 194,\n",
       " 'free': 195,\n",
       " 'through': 196,\n",
       " 'real': 197,\n",
       " 'english': 198,\n",
       " 'look': 199,\n",
       " 'black': 200,\n",
       " 'american': 201,\n",
       " 'war': 202,\n",
       " '3': 203,\n",
       " 'true': 204,\n",
       " 'experience': 205,\n",
       " 'such': 206,\n",
       " 'questions': 207,\n",
       " 'come': 208,\n",
       " 'doing': 209,\n",
       " '2018': 210,\n",
       " 'system': 211,\n",
       " 'class': 212,\n",
       " 'government': 213,\n",
       " 'students': 214,\n",
       " 'man': 215,\n",
       " 'water': 216,\n",
       " 'number': 217,\n",
       " 'important': 218,\n",
       " 'against': 219,\n",
       " 'very': 220,\n",
       " 'language': 221,\n",
       " 'ways': 222,\n",
       " 'state': 223,\n",
       " 'books': 224,\n",
       " 'off': 225,\n",
       " 'computer': 226,\n",
       " 'tell': 227,\n",
       " 'friend': 228,\n",
       " 'keep': 229,\n",
       " 'human': 230,\n",
       " 'never': 231,\n",
       " 'girls': 232,\n",
       " 'social': 233,\n",
       " 'always': 234,\n",
       " 'every': 235,\n",
       " '10': 236,\n",
       " 'too': 237,\n",
       " '5': 238,\n",
       " 'home': 239,\n",
       " 'career': 240,\n",
       " 'data': 241,\n",
       " 'non': 242,\n",
       " 'chinese': 243,\n",
       " 'relationship': 244,\n",
       " 'place': 245,\n",
       " 'america': 246,\n",
       " 'friends': 247,\n",
       " 'under': 248,\n",
       " 'doesn': 249,\n",
       " 'software': 250,\n",
       " 'write': 251,\n",
       " 'future': 252,\n",
       " 'done': 253,\n",
       " 'working': 254,\n",
       " 'exam': 255,\n",
       " 'learning': 256,\n",
       " 'phone': 257,\n",
       " 'account': 258,\n",
       " 've': 259,\n",
       " 'parents': 260,\n",
       " 'family': 261,\n",
       " 'website': 262,\n",
       " 'app': 263,\n",
       " 'guy': 264,\n",
       " 'food': 265,\n",
       " 'another': 266,\n",
       " 'top': 267,\n",
       " 'usa': 268,\n",
       " 'got': 269,\n",
       " 'car': 270,\n",
       " 'age': 271,\n",
       " 'states': 272,\n",
       " 'around': 273,\n",
       " 'believe': 274,\n",
       " 'days': 275,\n",
       " 'google': 276,\n",
       " 'read': 277,\n",
       " 'happens': 278,\n",
       " 'course': 279,\n",
       " 'big': 280,\n",
       " 'wrong': 281,\n",
       " 'kind': 282,\n",
       " 'each': 283,\n",
       " 'companies': 284,\n",
       " 'down': 285,\n",
       " 'power': 286,\n",
       " '4': 287,\n",
       " 'body': 288,\n",
       " 'hard': 289,\n",
       " 'show': 290,\n",
       " 'americans': 291,\n",
       " 'able': 292,\n",
       " 'major': 293,\n",
       " 'makes': 294,\n",
       " 'today': 295,\n",
       " 'also': 296,\n",
       " 'movie': 297,\n",
       " 'history': 298,\n",
       " 'well': 299,\n",
       " 'last': 300,\n",
       " 'canada': 301,\n",
       " 'hate': 302,\n",
       " 'woman': 303,\n",
       " 'question': 304,\n",
       " 'god': 305,\n",
       " 'ask': 306,\n",
       " 'muslims': 307,\n",
       " 'cost': 308,\n",
       " 'less': 309,\n",
       " 'both': 310,\n",
       " 'president': 311,\n",
       " 'living': 312,\n",
       " 'normal': 313,\n",
       " 'jee': 314,\n",
       " 'considered': 315,\n",
       " 'pay': 316,\n",
       " 'type': 317,\n",
       " 'child': 318,\n",
       " 'create': 319,\n",
       " 'degree': 320,\n",
       " 'test': 321,\n",
       " 'children': 322,\n",
       " 'facebook': 323,\n",
       " 'earth': 324,\n",
       " 'since': 325,\n",
       " 'part': 326,\n",
       " 'instead': 327,\n",
       " 'end': 328,\n",
       " 'media': 329,\n",
       " 'great': 330,\n",
       " 'examples': 331,\n",
       " 'play': 332,\n",
       " 'common': 333,\n",
       " 'answer': 334,\n",
       " 'prepare': 335,\n",
       " 'favorite': 336,\n",
       " 'rank': 337,\n",
       " 'market': 338,\n",
       " 'actually': 339,\n",
       " 'process': 340,\n",
       " 'law': 341,\n",
       " 'anything': 342,\n",
       " 'house': 343,\n",
       " 'united': 344,\n",
       " 'given': 345,\n",
       " 'tv': 346,\n",
       " 'eat': 347,\n",
       " 'win': 348,\n",
       " 'myself': 349,\n",
       " 'choose': 350,\n",
       " 'marks': 351,\n",
       " 'medical': 352,\n",
       " 're': 353,\n",
       " 'music': 354,\n",
       " 'next': 355,\n",
       " 'average': 356,\n",
       " 'known': 357,\n",
       " 'call': 358,\n",
       " 'jobs': 359,\n",
       " 'game': 360,\n",
       " 'making': 361,\n",
       " 'writing': 362,\n",
       " 'order': 363,\n",
       " 'cause': 364,\n",
       " 'uk': 365,\n",
       " 'current': 366,\n",
       " 'score': 367,\n",
       " 'small': 368,\n",
       " 'indians': 369,\n",
       " 'video': 370,\n",
       " 'north': 371,\n",
       " 'join': 372,\n",
       " 'affect': 373,\n",
       " 'service': 374,\n",
       " 'apply': 375,\n",
       " 'deal': 376,\n",
       " 'reason': 377,\n",
       " 'self': 378,\n",
       " 'main': 379,\n",
       " 'causes': 380,\n",
       " 'support': 381,\n",
       " 'worth': 382,\n",
       " 'bank': 383,\n",
       " 'these': 384,\n",
       " 'called': 385,\n",
       " 'left': 386,\n",
       " 'form': 387,\n",
       " 'south': 388,\n",
       " 'tips': 389,\n",
       " 'point': 390,\n",
       " 'based': 391,\n",
       " 'pakistan': 392,\n",
       " 'technology': 393,\n",
       " 'face': 394,\n",
       " 'word': 395,\n",
       " 'put': 396,\n",
       " 'public': 397,\n",
       " 'muslim': 398,\n",
       " 'delhi': 399,\n",
       " 'engineer': 400,\n",
       " 'energy': 401,\n",
       " 'open': 402,\n",
       " 'others': 403,\n",
       " 'card': 404,\n",
       " 'iit': 405,\n",
       " 'months': 406,\n",
       " 'travel': 407,\n",
       " 'etc': 408,\n",
       " 'management': 409,\n",
       " 'value': 410,\n",
       " 'marketing': 411,\n",
       " 'those': 412,\n",
       " 'light': 413,\n",
       " 'move': 414,\n",
       " 'taking': 415,\n",
       " 'research': 416,\n",
       " 'level': 417,\n",
       " 'hair': 418,\n",
       " 'full': 419,\n",
       " 'improve': 420,\n",
       " 'development': 421,\n",
       " 'design': 422,\n",
       " 'lot': 423,\n",
       " 'control': 424,\n",
       " 'meaning': 425,\n",
       " 'date': 426,\n",
       " 'problem': 427,\n",
       " 'times': 428,\n",
       " 'idea': 429,\n",
       " 'found': 430,\n",
       " 'away': 431,\n",
       " 'education': 432,\n",
       " 'safe': 433,\n",
       " 'city': 434,\n",
       " 'program': 435,\n",
       " 'single': 436,\n",
       " 'term': 437,\n",
       " 'health': 438,\n",
       " 'air': 439,\n",
       " 'care': 440,\n",
       " 'series': 441,\n",
       " 'physics': 442,\n",
       " 'general': 443,\n",
       " 'space': 444,\n",
       " 'international': 445,\n",
       " 'low': 446,\n",
       " 'political': 447,\n",
       " 'weight': 448,\n",
       " 'enough': 449,\n",
       " 'donald': 450,\n",
       " 'required': 451,\n",
       " 'android': 452,\n",
       " 'stay': 453,\n",
       " 'understand': 454,\n",
       " '12': 455,\n",
       " 'field': 456,\n",
       " 'talk': 457,\n",
       " 'mba': 458,\n",
       " 'movies': 459,\n",
       " 'increase': 460,\n",
       " 'im': 461,\n",
       " 'month': 462,\n",
       " 'everyone': 463,\n",
       " 'popular': 464,\n",
       " 'problems': 465,\n",
       " 'isn': 466,\n",
       " 'programming': 467,\n",
       " 'youtube': 468,\n",
       " 'dog': 469,\n",
       " 'group': 470,\n",
       " 'build': 471,\n",
       " 'humans': 472,\n",
       " 'behind': 473,\n",
       " 'legal': 474,\n",
       " 'salary': 475,\n",
       " 'boyfriend': 476,\n",
       " 'happened': 477,\n",
       " 'available': 478,\n",
       " 'post': 479,\n",
       " 'death': 480,\n",
       " 'mobile': 481,\n",
       " '6': 482,\n",
       " 'military': 483,\n",
       " 'c': 484,\n",
       " 'often': 485,\n",
       " 'again': 486,\n",
       " 'math': 487,\n",
       " 'said': 488,\n",
       " 'mind': 489,\n",
       " 'girlfriend': 490,\n",
       " 'short': 491,\n",
       " 'benefits': 492,\n",
       " 'story': 493,\n",
       " 'society': 494,\n",
       " 'female': 495,\n",
       " 'civil': 496,\n",
       " 'run': 497,\n",
       " 'russia': 498,\n",
       " 'looking': 499,\n",
       " 'didn': 500,\n",
       " 'kill': 501,\n",
       " 'culture': 502,\n",
       " 'ex': 503,\n",
       " 'leave': 504,\n",
       " 'wear': 505,\n",
       " 'successful': 506,\n",
       " 'else': 507,\n",
       " 'europe': 508,\n",
       " 'visa': 509,\n",
       " 'religion': 510,\n",
       " 'dont': 511,\n",
       " 'germany': 512,\n",
       " 'purpose': 513,\n",
       " 'party': 514,\n",
       " 'x': 515,\n",
       " 'due': 516,\n",
       " 'let': 517,\n",
       " 'side': 518,\n",
       " 'mass': 519,\n",
       " 'per': 520,\n",
       " 'role': 521,\n",
       " 'web': 522,\n",
       " 'related': 523,\n",
       " 'kids': 524,\n",
       " 'information': 525,\n",
       " 'male': 526,\n",
       " 'admission': 527,\n",
       " 'answers': 528,\n",
       " 'die': 529,\n",
       " 'past': 530,\n",
       " 'consider': 531,\n",
       " 'cat': 532,\n",
       " 'british': 533,\n",
       " 'speed': 534,\n",
       " 'internet': 535,\n",
       " 'try': 536,\n",
       " 'skills': 537,\n",
       " 'similar': 538,\n",
       " 'watch': 539,\n",
       " 'force': 540,\n",
       " 'starting': 541,\n",
       " 'colleges': 542,\n",
       " 'guys': 543,\n",
       " 'mechanical': 544,\n",
       " 'industry': 545,\n",
       " 'training': 546,\n",
       " 'everything': 547,\n",
       " 'product': 548,\n",
       " 'gay': 549,\n",
       " 'studying': 550,\n",
       " 'few': 551,\n",
       " 'pain': 552,\n",
       " 'seen': 553,\n",
       " 'modern': 554,\n",
       " 'lose': 555,\n",
       " 'opinion': 556,\n",
       " 'sell': 557,\n",
       " 'yourself': 558,\n",
       " 'korea': 559,\n",
       " 'site': 560,\n",
       " 'types': 561,\n",
       " 'advice': 562,\n",
       " 'share': 563,\n",
       " 'fight': 564,\n",
       " 'earn': 565,\n",
       " 'games': 566,\n",
       " 'tax': 567,\n",
       " 'second': 568,\n",
       " 'exist': 569,\n",
       " 'effective': 570,\n",
       " 'services': 571,\n",
       " 'night': 572,\n",
       " 'allowed': 573,\n",
       " 'though': 574,\n",
       " 'blood': 575,\n",
       " 'australia': 576,\n",
       " 'personal': 577,\n",
       " 'application': 578,\n",
       " 'interview': 579,\n",
       " '20': 580,\n",
       " 'machine': 581,\n",
       " 'rate': 582,\n",
       " 'mother': 583,\n",
       " 'themselves': 584,\n",
       " 'police': 585,\n",
       " 'area': 586,\n",
       " 'project': 587,\n",
       " 'rather': 588,\n",
       " 'foreign': 589,\n",
       " 'famous': 590,\n",
       " 'paper': 591,\n",
       " 'marriage': 592,\n",
       " 'bangalore': 593,\n",
       " 'courses': 594,\n",
       " 'case': 595,\n",
       " '100': 596,\n",
       " 'code': 597,\n",
       " 'terms': 598,\n",
       " 'set': 599,\n",
       " 'private': 600,\n",
       " 'period': 601,\n",
       " 'brain': 602,\n",
       " 'difficult': 603,\n",
       " 'already': 604,\n",
       " 'mumbai': 605,\n",
       " 'effect': 606,\n",
       " 'trying': 607,\n",
       " 'easy': 608,\n",
       " 'color': 609,\n",
       " 'compared': 610,\n",
       " '7': 611,\n",
       " 'liberals': 612,\n",
       " 'explain': 613,\n",
       " 'act': 614,\n",
       " 'amount': 615,\n",
       " 'daily': 616,\n",
       " 'price': 617,\n",
       " 'worst': 618,\n",
       " 'skin': 619,\n",
       " 'middle': 620,\n",
       " 'plan': 621,\n",
       " '15': 622,\n",
       " 'team': 623,\n",
       " 'words': 624,\n",
       " 'treat': 625,\n",
       " 'wants': 626,\n",
       " 'says': 627,\n",
       " 'neet': 628,\n",
       " 'wife': 629,\n",
       " 'universe': 630,\n",
       " 'japanese': 631,\n",
       " 'song': 632,\n",
       " 'hours': 633,\n",
       " 'towards': 634,\n",
       " 'news': 635,\n",
       " 'useful': 636,\n",
       " 'won': 637,\n",
       " 'married': 638,\n",
       " 'quality': 639,\n",
       " 'places': 640,\n",
       " 'function': 641,\n",
       " 'follow': 642,\n",
       " 'biggest': 643,\n",
       " 'products': 644,\n",
       " 'whats': 645,\n",
       " 'prevent': 646,\n",
       " 'b': 647,\n",
       " 'asked': 648,\n",
       " 'example': 649,\n",
       " 'knowledge': 650,\n",
       " 'sleep': 651,\n",
       " 'correct': 652,\n",
       " 'once': 653,\n",
       " 'seem': 654,\n",
       " 'happy': 655,\n",
       " 'young': 656,\n",
       " 'visit': 657,\n",
       " 'yet': 658,\n",
       " 'turn': 659,\n",
       " 'theory': 660,\n",
       " 'iphone': 661,\n",
       " 'institute': 662,\n",
       " 'amazon': 663,\n",
       " 'rid': 664,\n",
       " 'ideas': 665,\n",
       " 'national': 666,\n",
       " 'taken': 667,\n",
       " 'three': 668,\n",
       " 'started': 669,\n",
       " 'cse': 670,\n",
       " 'coaching': 671,\n",
       " 'model': 672,\n",
       " 'lost': 673,\n",
       " 'vs': 674,\n",
       " 'israel': 675,\n",
       " 'star': 676,\n",
       " 'describe': 677,\n",
       " 'islam': 678,\n",
       " 'offer': 679,\n",
       " 'may': 680,\n",
       " 'view': 681,\n",
       " 'animals': 682,\n",
       " 'gun': 683,\n",
       " 'effects': 684,\n",
       " 'list': 685,\n",
       " 'invest': 686,\n",
       " 'oil': 687,\n",
       " 'download': 688,\n",
       " 'modi': 689,\n",
       " '12th': 690,\n",
       " 'red': 691,\n",
       " 'stock': 692,\n",
       " 'japan': 693,\n",
       " 'differences': 694,\n",
       " 'far': 695,\n",
       " 'line': 696,\n",
       " 'okay': 697,\n",
       " 'avoid': 698,\n",
       " 'week': 699,\n",
       " 'matter': 700,\n",
       " 'instagram': 701,\n",
       " 'negative': 702,\n",
       " 'fast': 703,\n",
       " 'character': 704,\n",
       " 'father': 705,\n",
       " 'thinking': 706,\n",
       " 'phd': 707,\n",
       " 'little': 708,\n",
       " 'eating': 709,\n",
       " 'expect': 710,\n",
       " 'natural': 711,\n",
       " 'security': 712,\n",
       " 'sound': 713,\n",
       " 'develop': 714,\n",
       " 'jews': 715,\n",
       " 'chemical': 716,\n",
       " 'chance': 717,\n",
       " 'provide': 718,\n",
       " 'necessary': 719,\n",
       " 'reasons': 720,\n",
       " 'anti': 721,\n",
       " 'must': 722,\n",
       " 'board': 723,\n",
       " 'exams': 724,\n",
       " 'laptop': 725,\n",
       " 'grow': 726,\n",
       " '8': 727,\n",
       " 'likely': 728,\n",
       " 'sexual': 729,\n",
       " 'size': 730,\n",
       " 'center': 731,\n",
       " 'professional': 732,\n",
       " 'depression': 733,\n",
       " 'present': 734,\n",
       " 'remove': 735,\n",
       " 'add': 736,\n",
       " 'facts': 737,\n",
       " 'chances': 738,\n",
       " 'feeling': 739,\n",
       " 'army': 740,\n",
       " '11': 741,\n",
       " 'large': 742,\n",
       " 'reading': 743,\n",
       " 'accept': 744,\n",
       " 'within': 745,\n",
       " 'schools': 746,\n",
       " 'send': 747,\n",
       " 'cut': 748,\n",
       " 'apps': 749,\n",
       " 'dream': 750,\n",
       " 'option': 751,\n",
       " 'told': 752,\n",
       " 'speak': 753,\n",
       " 'together': 754,\n",
       " 'office': 755,\n",
       " 'prefer': 756,\n",
       " 'moving': 757,\n",
       " 'european': 758,\n",
       " 'boy': 759,\n",
       " 'husband': 760,\n",
       " 'poor': 761,\n",
       " 'd': 762,\n",
       " 'preparation': 763,\n",
       " 'becoming': 764,\n",
       " 'fake': 765,\n",
       " 'interested': 766,\n",
       " 'investment': 767,\n",
       " 'head': 768,\n",
       " 'fall': 769,\n",
       " 'least': 770,\n",
       " 'graduate': 771,\n",
       " 'digital': 772,\n",
       " 'break': 773,\n",
       " 'financial': 774,\n",
       " 'close': 775,\n",
       " 'obama': 776,\n",
       " 'nothing': 777,\n",
       " 'interesting': 778,\n",
       " 'paid': 779,\n",
       " 'income': 780,\n",
       " 'mental': 781,\n",
       " 'western': 782,\n",
       " 'thoughts': 783,\n",
       " 'here': 784,\n",
       " 'dating': 785,\n",
       " 'illegal': 786,\n",
       " 'options': 787,\n",
       " 'percentage': 788,\n",
       " 'impact': 789,\n",
       " 'nuclear': 790,\n",
       " 'outside': 791,\n",
       " 'contact': 792,\n",
       " 'subject': 793,\n",
       " 'drive': 794,\n",
       " 'basic': 795,\n",
       " 'french': 796,\n",
       " 'universities': 797,\n",
       " 'upsc': 798,\n",
       " '30': 799,\n",
       " 'languages': 800,\n",
       " 'gain': 801,\n",
       " 'allow': 802,\n",
       " 'comes': 803,\n",
       " 'higher': 804,\n",
       " 'store': 805,\n",
       " 'uses': 806,\n",
       " 'studies': 807,\n",
       " 'baby': 808,\n",
       " 'land': 809,\n",
       " 'population': 810,\n",
       " 'aren': 811,\n",
       " 'java': 812,\n",
       " 'style': 813,\n",
       " 'physical': 814,\n",
       " 'might': 815,\n",
       " 'rights': 816,\n",
       " 'born': 817,\n",
       " 'interest': 818,\n",
       " 'special': 819,\n",
       " 'hand': 820,\n",
       " 'teacher': 821,\n",
       " 'chemistry': 822,\n",
       " 'gate': 823,\n",
       " 'advantages': 824,\n",
       " 'solution': 825,\n",
       " 'mains': 826,\n",
       " 'characteristics': 827,\n",
       " 'personality': 828,\n",
       " 'ms': 829,\n",
       " 'websites': 830,\n",
       " 'check': 831,\n",
       " 'racist': 832,\n",
       " 'german': 833,\n",
       " 'changed': 834,\n",
       " 'issues': 835,\n",
       " 'intelligence': 836,\n",
       " 'son': 837,\n",
       " 'email': 838,\n",
       " 'fact': 839,\n",
       " 'heart': 840,\n",
       " 'developer': 841,\n",
       " 'gets': 842,\n",
       " 'doctor': 843,\n",
       " '18': 844,\n",
       " 'rich': 845,\n",
       " 'apple': 846,\n",
       " 'room': 847,\n",
       " 'created': 848,\n",
       " 'strong': 849,\n",
       " 'recommend': 850,\n",
       " 'meet': 851,\n",
       " 'west': 852,\n",
       " 'fat': 853,\n",
       " '9': 854,\n",
       " 'russian': 855,\n",
       " 'pressure': 856,\n",
       " 'please': 857,\n",
       " 'branch': 858,\n",
       " 'credit': 859,\n",
       " 'economy': 860,\n",
       " 'sites': 861,\n",
       " 'pregnant': 862,\n",
       " 'marry': 863,\n",
       " 'hindi': 864,\n",
       " 'startup': 865,\n",
       " 'dark': 866,\n",
       " 'laws': 867,\n",
       " 'currently': 868,\n",
       " 'positive': 869,\n",
       " 'pass': 870,\n",
       " 'structure': 871,\n",
       " 'charge': 872,\n",
       " 'source': 873,\n",
       " 'ones': 874,\n",
       " 'pursue': 875,\n",
       " 'near': 876,\n",
       " 'africa': 877,\n",
       " 'inside': 878,\n",
       " 'practice': 879,\n",
       " 'partner': 880,\n",
       " 'early': 881,\n",
       " 'solve': 882,\n",
       " 'hyderabad': 883,\n",
       " '50': 884,\n",
       " 'scope': 885,\n",
       " 'spend': 886,\n",
       " 'building': 887,\n",
       " 'smart': 888,\n",
       " 'network': 889,\n",
       " 'crush': 890,\n",
       " 'bitcoin': 891,\n",
       " 'talking': 892,\n",
       " 'bring': 893,\n",
       " 'christians': 894,\n",
       " 'ok': 895,\n",
       " 'views': 896,\n",
       " 'whole': 897,\n",
       " 'art': 898,\n",
       " 'videos': 899,\n",
       " 'mom': 900,\n",
       " 'completely': 901,\n",
       " 'numbers': 902,\n",
       " 'windows': 903,\n",
       " 'iq': 904,\n",
       " 'maths': 905,\n",
       " 'officer': 906,\n",
       " 'topics': 907,\n",
       " 'alone': 908,\n",
       " 'ca': 909,\n",
       " 'dogs': 910,\n",
       " 'film': 911,\n",
       " 'among': 912,\n",
       " 'certain': 913,\n",
       " 'pune': 914,\n",
       " 'cell': 915,\n",
       " 'healthy': 916,\n",
       " 'content': 917,\n",
       " 'global': 918,\n",
       " 'cbse': 919,\n",
       " 'christian': 920,\n",
       " 'especially': 921,\n",
       " 'gas': 922,\n",
       " 'works': 923,\n",
       " 'factors': 924,\n",
       " 'complete': 925,\n",
       " 'exactly': 926,\n",
       " 'yes': 927,\n",
       " 'according': 928,\n",
       " 'written': 929,\n",
       " 'needed': 930,\n",
       " 'tech': 931,\n",
       " 'fear': 932,\n",
       " 'distance': 933,\n",
       " 'asian': 934,\n",
       " 'stupid': 935,\n",
       " 'save': 936,\n",
       " 'religious': 937,\n",
       " 'text': 938,\n",
       " 'e': 939,\n",
       " 'reduce': 940,\n",
       " 'cold': 941,\n",
       " 'songs': 942,\n",
       " 'treatment': 943,\n",
       " 'ago': 944,\n",
       " 'east': 945,\n",
       " 'master': 946,\n",
       " 'electrical': 947,\n",
       " 'file': 948,\n",
       " 'playing': 949,\n",
       " 'coming': 950,\n",
       " 'race': 951,\n",
       " 'hurt': 952,\n",
       " 'insurance': 953,\n",
       " 'eyes': 954,\n",
       " 'giving': 955,\n",
       " 'attack': 956,\n",
       " 'buying': 957,\n",
       " 'community': 958,\n",
       " 'novel': 959,\n",
       " 'train': 960,\n",
       " 'teach': 961,\n",
       " 'democrats': 962,\n",
       " 'advanced': 963,\n",
       " 'minimum': 964,\n",
       " 'policy': 965,\n",
       " 'simple': 966,\n",
       " 'african': 967,\n",
       " 'letter': 968,\n",
       " 'opportunities': 969,\n",
       " 'fix': 970,\n",
       " 'dead': 971,\n",
       " 'cs': 972,\n",
       " 'multiple': 973,\n",
       " 'thought': 974,\n",
       " 'hindu': 975,\n",
       " 'sense': 976,\n",
       " 'running': 977,\n",
       " 'mathematics': 978,\n",
       " 'eye': 979,\n",
       " 'result': 980,\n",
       " 'procedure': 981,\n",
       " 'almost': 982,\n",
       " 'method': 983,\n",
       " 'easily': 984,\n",
       " 'abroad': 985,\n",
       " 'green': 986,\n",
       " 'boys': 987,\n",
       " 'anxiety': 988,\n",
       " 'transfer': 989,\n",
       " 'lead': 990,\n",
       " 'front': 991,\n",
       " 'football': 992,\n",
       " 'react': 993,\n",
       " 'clear': 994,\n",
       " 'lower': 995,\n",
       " 'half': 996,\n",
       " 'search': 997,\n",
       " 'economic': 998,\n",
       " 'importance': 999,\n",
       " 'cars': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " 'what': 1,\n",
       " 'is': 2,\n",
       " 'a': 3,\n",
       " 'to': 4,\n",
       " 'in': 5,\n",
       " 'of': 6,\n",
       " 'i': 7,\n",
       " 'how': 8,\n",
       " 'and': 9,\n",
       " 'do': 10,\n",
       " 'are': 11,\n",
       " 'for': 12,\n",
       " 'you': 13,\n",
       " 'can': 14,\n",
       " 'why': 15,\n",
       " 'it': 16,\n",
       " 'my': 17,\n",
       " 'that': 18,\n",
       " 'if': 19,\n",
       " 'with': 20,\n",
       " 'on': 21,\n",
       " 'or': 22,\n",
       " 'have': 23,\n",
       " 'be': 24,\n",
       " 'does': 25,\n",
       " 's': 26,\n",
       " 'from': 27,\n",
       " 'your': 28,\n",
       " 'an': 29,\n",
       " 'which': 30,\n",
       " 'should': 31,\n",
       " 'when': 32,\n",
       " 'get': 33,\n",
       " 'best': 34,\n",
       " 'would': 35,\n",
       " 'as': 36,\n",
       " 'people': 37,\n",
       " 'some': 38,\n",
       " 'there': 39,\n",
       " 'who': 40,\n",
       " 'will': 41,\n",
       " 'like': 42,\n",
       " 'at': 43,\n",
       " 'not': 44,\n",
       " 't': 45,\n",
       " 'about': 46,\n",
       " 'they': 47,\n",
       " 'by': 48,\n",
       " 'did': 49,\n",
       " 'was': 50,\n",
       " 'we': 51,\n",
       " 'any': 52,\n",
       " 'so': 53,\n",
       " 'good': 54,\n",
       " 'me': 55,\n",
       " 'their': 56,\n",
       " 'one': 57,\n",
       " 'india': 58,\n",
       " 'has': 59,\n",
       " 'after': 60,\n",
       " 'most': 61,\n",
       " 'where': 62,\n",
       " 'make': 63,\n",
       " 'this': 64,\n",
       " 'but': 65,\n",
       " 'more': 66,\n",
       " 'all': 67,\n",
       " 'think': 68,\n",
       " 'many': 69,\n",
       " 'between': 70,\n",
       " 'time': 71,\n",
       " 'than': 72,\n",
       " 'much': 73,\n",
       " 'other': 74,\n",
       " 'someone': 75,\n",
       " 'he': 76,\n",
       " 'life': 77,\n",
       " 'use': 78,\n",
       " 'out': 79,\n",
       " 'way': 80,\n",
       " 'am': 81,\n",
       " 'us': 82,\n",
       " 'know': 83,\n",
       " 'up': 84,\n",
       " 'being': 85,\n",
       " 'work': 86,\n",
       " 'want': 87,\n",
       " 'take': 88,\n",
       " 'them': 89,\n",
       " 'were': 90,\n",
       " 'ever': 91,\n",
       " 'world': 92,\n",
       " 'his': 93,\n",
       " 'find': 94,\n",
       " 'become': 95,\n",
       " 'without': 96,\n",
       " 'just': 97,\n",
       " 'person': 98,\n",
       " 'could': 99,\n",
       " 'don': 100,\n",
       " 'feel': 101,\n",
       " 'year': 102,\n",
       " 'into': 103,\n",
       " 'better': 104,\n",
       " 'quora': 105,\n",
       " 'no': 106,\n",
       " 'go': 107,\n",
       " 'new': 108,\n",
       " 'm': 109,\n",
       " 'trump': 110,\n",
       " 'possible': 111,\n",
       " 'job': 112,\n",
       " 'only': 113,\n",
       " 'her': 114,\n",
       " 'years': 115,\n",
       " 'been': 116,\n",
       " 'indian': 117,\n",
       " 'mean': 118,\n",
       " 'women': 119,\n",
       " 'used': 120,\n",
       " 'need': 121,\n",
       " 'start': 122,\n",
       " 'first': 123,\n",
       " 'had': 124,\n",
       " 'difference': 125,\n",
       " 'money': 126,\n",
       " 'still': 127,\n",
       " 'different': 128,\n",
       " 'school': 129,\n",
       " 'country': 130,\n",
       " 'really': 131,\n",
       " 'long': 132,\n",
       " 'while': 133,\n",
       " 'old': 134,\n",
       " 'over': 135,\n",
       " 'our': 136,\n",
       " 'learn': 137,\n",
       " 'business': 138,\n",
       " 'same': 139,\n",
       " 'now': 140,\n",
       " 'love': 141,\n",
       " 'its': 142,\n",
       " 'before': 143,\n",
       " 'college': 144,\n",
       " 'things': 145,\n",
       " '2': 146,\n",
       " 'she': 147,\n",
       " 'even': 148,\n",
       " 'engineering': 149,\n",
       " 'give': 150,\n",
       " 'help': 151,\n",
       " 'during': 152,\n",
       " 'online': 153,\n",
       " 'day': 154,\n",
       " 'say': 155,\n",
       " 'see': 156,\n",
       " 'men': 157,\n",
       " 'bad': 158,\n",
       " 'because': 159,\n",
       " 'using': 160,\n",
       " 'book': 161,\n",
       " 'university': 162,\n",
       " 'company': 163,\n",
       " 'back': 164,\n",
       " 'change': 165,\n",
       " 'high': 166,\n",
       " 'sex': 167,\n",
       " 'then': 168,\n",
       " 'him': 169,\n",
       " 'anyone': 170,\n",
       " 'made': 171,\n",
       " 'stop': 172,\n",
       " 'girl': 173,\n",
       " 'having': 174,\n",
       " 'getting': 175,\n",
       " 'right': 176,\n",
       " '2017': 177,\n",
       " 'student': 178,\n",
       " 'live': 179,\n",
       " 'buy': 180,\n",
       " '1': 181,\n",
       " 'two': 182,\n",
       " 'study': 183,\n",
       " 'own': 184,\n",
       " 'thing': 185,\n",
       " 'happen': 186,\n",
       " 'science': 187,\n",
       " 'going': 188,\n",
       " 'name': 189,\n",
       " 'countries': 190,\n",
       " 'white': 191,\n",
       " 'something': 192,\n",
       " 'china': 193,\n",
       " 'free': 194,\n",
       " 'through': 195,\n",
       " 'real': 196,\n",
       " 'english': 197,\n",
       " 'look': 198,\n",
       " 'black': 199,\n",
       " 'american': 200,\n",
       " 'war': 201,\n",
       " '3': 202,\n",
       " 'true': 203,\n",
       " 'experience': 204,\n",
       " 'such': 205,\n",
       " 'questions': 206,\n",
       " 'come': 207,\n",
       " 'doing': 208,\n",
       " '2018': 209,\n",
       " 'system': 210,\n",
       " 'class': 211,\n",
       " 'government': 212,\n",
       " 'students': 213,\n",
       " 'man': 214,\n",
       " 'water': 215,\n",
       " 'number': 216,\n",
       " 'important': 217,\n",
       " 'against': 218,\n",
       " 'very': 219,\n",
       " 'language': 220,\n",
       " 'ways': 221,\n",
       " 'state': 222,\n",
       " 'books': 223,\n",
       " 'off': 224,\n",
       " 'computer': 225,\n",
       " 'tell': 226,\n",
       " 'friend': 227,\n",
       " 'keep': 228,\n",
       " 'human': 229,\n",
       " 'never': 230,\n",
       " 'girls': 231,\n",
       " 'social': 232,\n",
       " 'always': 233,\n",
       " 'every': 234,\n",
       " '10': 235,\n",
       " 'too': 236,\n",
       " '5': 237,\n",
       " 'home': 238,\n",
       " 'career': 239,\n",
       " 'data': 240,\n",
       " 'non': 241,\n",
       " 'chinese': 242,\n",
       " 'relationship': 243,\n",
       " 'place': 244,\n",
       " 'america': 245,\n",
       " 'friends': 246,\n",
       " 'under': 247,\n",
       " 'doesn': 248,\n",
       " 'software': 249,\n",
       " 'write': 250,\n",
       " 'future': 251,\n",
       " 'done': 252,\n",
       " 'working': 253,\n",
       " 'exam': 254,\n",
       " 'learning': 255,\n",
       " 'phone': 256,\n",
       " 'account': 257,\n",
       " 've': 258,\n",
       " 'parents': 259,\n",
       " 'family': 260,\n",
       " 'website': 261,\n",
       " 'app': 262,\n",
       " 'guy': 263,\n",
       " 'food': 264,\n",
       " 'another': 265,\n",
       " 'top': 266,\n",
       " 'usa': 267,\n",
       " 'got': 268,\n",
       " 'car': 269,\n",
       " 'age': 270,\n",
       " 'states': 271,\n",
       " 'around': 272,\n",
       " 'believe': 273,\n",
       " 'days': 274,\n",
       " 'google': 275,\n",
       " 'read': 276,\n",
       " 'happens': 277,\n",
       " 'course': 278,\n",
       " 'big': 279,\n",
       " 'wrong': 280,\n",
       " 'kind': 281,\n",
       " 'each': 282,\n",
       " 'companies': 283,\n",
       " 'down': 284,\n",
       " 'power': 285,\n",
       " '4': 286,\n",
       " 'body': 287,\n",
       " 'hard': 288,\n",
       " 'show': 289,\n",
       " 'americans': 290,\n",
       " 'able': 291,\n",
       " 'major': 292,\n",
       " 'makes': 293,\n",
       " 'today': 294,\n",
       " 'also': 295,\n",
       " 'movie': 296,\n",
       " 'history': 297,\n",
       " 'well': 298,\n",
       " 'last': 299,\n",
       " 'canada': 300,\n",
       " 'hate': 301,\n",
       " 'woman': 302,\n",
       " 'question': 303,\n",
       " 'god': 304,\n",
       " 'ask': 305,\n",
       " 'muslims': 306,\n",
       " 'cost': 307,\n",
       " 'less': 308,\n",
       " 'both': 309,\n",
       " 'president': 310,\n",
       " 'living': 311,\n",
       " 'normal': 312,\n",
       " 'jee': 313,\n",
       " 'considered': 314,\n",
       " 'pay': 315,\n",
       " 'type': 316,\n",
       " 'child': 317,\n",
       " 'create': 318,\n",
       " 'degree': 319,\n",
       " 'test': 320,\n",
       " 'children': 321,\n",
       " 'facebook': 322,\n",
       " 'earth': 323,\n",
       " 'since': 324,\n",
       " 'part': 325,\n",
       " 'instead': 326,\n",
       " 'end': 327,\n",
       " 'media': 328,\n",
       " 'great': 329,\n",
       " 'examples': 330,\n",
       " 'play': 331,\n",
       " 'common': 332,\n",
       " 'answer': 333,\n",
       " 'prepare': 334,\n",
       " 'favorite': 335,\n",
       " 'rank': 336,\n",
       " 'market': 337,\n",
       " 'actually': 338,\n",
       " 'process': 339,\n",
       " 'law': 340,\n",
       " 'anything': 341,\n",
       " 'house': 342,\n",
       " 'united': 343,\n",
       " 'given': 344,\n",
       " 'tv': 345,\n",
       " 'eat': 346,\n",
       " 'win': 347,\n",
       " 'myself': 348,\n",
       " 'choose': 349,\n",
       " 'marks': 350,\n",
       " 'medical': 351,\n",
       " 're': 352,\n",
       " 'music': 353,\n",
       " 'next': 354,\n",
       " 'average': 355,\n",
       " 'known': 356,\n",
       " 'call': 357,\n",
       " 'jobs': 358,\n",
       " 'game': 359,\n",
       " 'making': 360,\n",
       " 'writing': 361,\n",
       " 'order': 362,\n",
       " 'cause': 363,\n",
       " 'uk': 364,\n",
       " 'current': 365,\n",
       " 'score': 366,\n",
       " 'small': 367,\n",
       " 'indians': 368,\n",
       " 'video': 369,\n",
       " 'north': 370,\n",
       " 'join': 371,\n",
       " 'affect': 372,\n",
       " 'service': 373,\n",
       " 'apply': 374,\n",
       " 'deal': 375,\n",
       " 'reason': 376,\n",
       " 'self': 377,\n",
       " 'main': 378,\n",
       " 'causes': 379,\n",
       " 'support': 380,\n",
       " 'worth': 381,\n",
       " 'bank': 382,\n",
       " 'these': 383,\n",
       " 'called': 384,\n",
       " 'left': 385,\n",
       " 'form': 386,\n",
       " 'south': 387,\n",
       " 'tips': 388,\n",
       " 'point': 389,\n",
       " 'based': 390,\n",
       " 'pakistan': 391,\n",
       " 'technology': 392,\n",
       " 'face': 393,\n",
       " 'word': 394,\n",
       " 'put': 395,\n",
       " 'public': 396,\n",
       " 'muslim': 397,\n",
       " 'delhi': 398,\n",
       " 'engineer': 399,\n",
       " 'energy': 400,\n",
       " 'open': 401,\n",
       " 'others': 402,\n",
       " 'card': 403,\n",
       " 'iit': 404,\n",
       " 'months': 405,\n",
       " 'travel': 406,\n",
       " 'etc': 407,\n",
       " 'management': 408,\n",
       " 'value': 409,\n",
       " 'marketing': 410,\n",
       " 'those': 411,\n",
       " 'light': 412,\n",
       " 'move': 413,\n",
       " 'taking': 414,\n",
       " 'research': 415,\n",
       " 'level': 416,\n",
       " 'hair': 417,\n",
       " 'full': 418,\n",
       " 'improve': 419,\n",
       " 'development': 420,\n",
       " 'design': 421,\n",
       " 'lot': 422,\n",
       " 'control': 423,\n",
       " 'meaning': 424,\n",
       " 'date': 425,\n",
       " 'problem': 426,\n",
       " 'times': 427,\n",
       " 'idea': 428,\n",
       " 'found': 429,\n",
       " 'away': 430,\n",
       " 'education': 431,\n",
       " 'safe': 432,\n",
       " 'city': 433,\n",
       " 'program': 434,\n",
       " 'single': 435,\n",
       " 'term': 436,\n",
       " 'health': 437,\n",
       " 'air': 438,\n",
       " 'care': 439,\n",
       " 'series': 440,\n",
       " 'physics': 441,\n",
       " 'general': 442,\n",
       " 'space': 443,\n",
       " 'international': 444,\n",
       " 'low': 445,\n",
       " 'political': 446,\n",
       " 'weight': 447,\n",
       " 'enough': 448,\n",
       " 'donald': 449,\n",
       " 'required': 450,\n",
       " 'android': 451,\n",
       " 'stay': 452,\n",
       " 'understand': 453,\n",
       " '12': 454,\n",
       " 'field': 455,\n",
       " 'talk': 456,\n",
       " 'mba': 457,\n",
       " 'movies': 458,\n",
       " 'increase': 459,\n",
       " 'im': 460,\n",
       " 'month': 461,\n",
       " 'everyone': 462,\n",
       " 'popular': 463,\n",
       " 'problems': 464,\n",
       " 'isn': 465,\n",
       " 'programming': 466,\n",
       " 'youtube': 467,\n",
       " 'dog': 468,\n",
       " 'group': 469,\n",
       " 'build': 470,\n",
       " 'humans': 471,\n",
       " 'behind': 472,\n",
       " 'legal': 473,\n",
       " 'salary': 474,\n",
       " 'boyfriend': 475,\n",
       " 'happened': 476,\n",
       " 'available': 477,\n",
       " 'post': 478,\n",
       " 'death': 479,\n",
       " 'mobile': 480,\n",
       " '6': 481,\n",
       " 'military': 482,\n",
       " 'c': 483,\n",
       " 'often': 484,\n",
       " 'again': 485,\n",
       " 'math': 486,\n",
       " 'said': 487,\n",
       " 'mind': 488,\n",
       " 'girlfriend': 489,\n",
       " 'short': 490,\n",
       " 'benefits': 491,\n",
       " 'story': 492,\n",
       " 'society': 493,\n",
       " 'female': 494,\n",
       " 'civil': 495,\n",
       " 'run': 496,\n",
       " 'russia': 497,\n",
       " 'looking': 498,\n",
       " 'didn': 499,\n",
       " 'kill': 500,\n",
       " 'culture': 501,\n",
       " 'ex': 502,\n",
       " 'leave': 503,\n",
       " 'wear': 504,\n",
       " 'successful': 505,\n",
       " 'else': 506,\n",
       " 'europe': 507,\n",
       " 'visa': 508,\n",
       " 'religion': 509,\n",
       " 'dont': 510,\n",
       " 'germany': 511,\n",
       " 'purpose': 512,\n",
       " 'party': 513,\n",
       " 'x': 514,\n",
       " 'due': 515,\n",
       " 'let': 516,\n",
       " 'side': 517,\n",
       " 'mass': 518,\n",
       " 'per': 519,\n",
       " 'role': 520,\n",
       " 'web': 521,\n",
       " 'related': 522,\n",
       " 'kids': 523,\n",
       " 'information': 524,\n",
       " 'male': 525,\n",
       " 'admission': 526,\n",
       " 'answers': 527,\n",
       " 'die': 528,\n",
       " 'past': 529,\n",
       " 'consider': 530,\n",
       " 'cat': 531,\n",
       " 'british': 532,\n",
       " 'speed': 533,\n",
       " 'internet': 534,\n",
       " 'try': 535,\n",
       " 'skills': 536,\n",
       " 'similar': 537,\n",
       " 'watch': 538,\n",
       " 'force': 539,\n",
       " 'starting': 540,\n",
       " 'colleges': 541,\n",
       " 'guys': 542,\n",
       " 'mechanical': 543,\n",
       " 'industry': 544,\n",
       " 'training': 545,\n",
       " 'everything': 546,\n",
       " 'product': 547,\n",
       " 'gay': 548,\n",
       " 'studying': 549,\n",
       " 'few': 550,\n",
       " 'pain': 551,\n",
       " 'seen': 552,\n",
       " 'modern': 553,\n",
       " 'lose': 554,\n",
       " 'opinion': 555,\n",
       " 'sell': 556,\n",
       " 'yourself': 557,\n",
       " 'korea': 558,\n",
       " 'site': 559,\n",
       " 'types': 560,\n",
       " 'advice': 561,\n",
       " 'share': 562,\n",
       " 'fight': 563,\n",
       " 'earn': 564,\n",
       " 'games': 565,\n",
       " 'tax': 566,\n",
       " 'second': 567,\n",
       " 'exist': 568,\n",
       " 'effective': 569,\n",
       " 'services': 570,\n",
       " 'night': 571,\n",
       " 'allowed': 572,\n",
       " 'though': 573,\n",
       " 'blood': 574,\n",
       " 'australia': 575,\n",
       " 'personal': 576,\n",
       " 'application': 577,\n",
       " 'interview': 578,\n",
       " '20': 579,\n",
       " 'machine': 580,\n",
       " 'rate': 581,\n",
       " 'mother': 582,\n",
       " 'themselves': 583,\n",
       " 'police': 584,\n",
       " 'area': 585,\n",
       " 'project': 586,\n",
       " 'rather': 587,\n",
       " 'foreign': 588,\n",
       " 'famous': 589,\n",
       " 'paper': 590,\n",
       " 'marriage': 591,\n",
       " 'bangalore': 592,\n",
       " 'courses': 593,\n",
       " 'case': 594,\n",
       " '100': 595,\n",
       " 'code': 596,\n",
       " 'terms': 597,\n",
       " 'set': 598,\n",
       " 'private': 599,\n",
       " 'period': 600,\n",
       " 'brain': 601,\n",
       " 'difficult': 602,\n",
       " 'already': 603,\n",
       " 'mumbai': 604,\n",
       " 'effect': 605,\n",
       " 'trying': 606,\n",
       " 'easy': 607,\n",
       " 'color': 608,\n",
       " 'compared': 609,\n",
       " '7': 610,\n",
       " 'liberals': 611,\n",
       " 'explain': 612,\n",
       " 'act': 613,\n",
       " 'amount': 614,\n",
       " 'daily': 615,\n",
       " 'price': 616,\n",
       " 'worst': 617,\n",
       " 'skin': 618,\n",
       " 'middle': 619,\n",
       " 'plan': 620,\n",
       " '15': 621,\n",
       " 'team': 622,\n",
       " 'words': 623,\n",
       " 'treat': 624,\n",
       " 'wants': 625,\n",
       " 'says': 626,\n",
       " 'neet': 627,\n",
       " 'wife': 628,\n",
       " 'universe': 629,\n",
       " 'japanese': 630,\n",
       " 'song': 631,\n",
       " 'hours': 632,\n",
       " 'towards': 633,\n",
       " 'news': 634,\n",
       " 'useful': 635,\n",
       " 'won': 636,\n",
       " 'married': 637,\n",
       " 'quality': 638,\n",
       " 'places': 639,\n",
       " 'function': 640,\n",
       " 'follow': 641,\n",
       " 'biggest': 642,\n",
       " 'products': 643,\n",
       " 'whats': 644,\n",
       " 'prevent': 645,\n",
       " 'b': 646,\n",
       " 'asked': 647,\n",
       " 'example': 648,\n",
       " 'knowledge': 649,\n",
       " 'sleep': 650,\n",
       " 'correct': 651,\n",
       " 'once': 652,\n",
       " 'seem': 653,\n",
       " 'happy': 654,\n",
       " 'young': 655,\n",
       " 'visit': 656,\n",
       " 'yet': 657,\n",
       " 'turn': 658,\n",
       " 'theory': 659,\n",
       " 'iphone': 660,\n",
       " 'institute': 661,\n",
       " 'amazon': 662,\n",
       " 'rid': 663,\n",
       " 'ideas': 664,\n",
       " 'national': 665,\n",
       " 'taken': 666,\n",
       " 'three': 667,\n",
       " 'started': 668,\n",
       " 'cse': 669,\n",
       " 'coaching': 670,\n",
       " 'model': 671,\n",
       " 'lost': 672,\n",
       " 'vs': 673,\n",
       " 'israel': 674,\n",
       " 'star': 675,\n",
       " 'describe': 676,\n",
       " 'islam': 677,\n",
       " 'offer': 678,\n",
       " 'may': 679,\n",
       " 'view': 680,\n",
       " 'animals': 681,\n",
       " 'gun': 682,\n",
       " 'effects': 683,\n",
       " 'list': 684,\n",
       " 'invest': 685,\n",
       " 'oil': 686,\n",
       " 'download': 687,\n",
       " 'modi': 688,\n",
       " '12th': 689,\n",
       " 'red': 690,\n",
       " 'stock': 691,\n",
       " 'japan': 692,\n",
       " 'differences': 693,\n",
       " 'far': 694,\n",
       " 'line': 695,\n",
       " 'okay': 696,\n",
       " 'avoid': 697,\n",
       " 'week': 698,\n",
       " 'matter': 699,\n",
       " 'instagram': 700,\n",
       " 'negative': 701,\n",
       " 'fast': 702,\n",
       " 'character': 703,\n",
       " 'father': 704,\n",
       " 'thinking': 705,\n",
       " 'phd': 706,\n",
       " 'little': 707,\n",
       " 'eating': 708,\n",
       " 'expect': 709,\n",
       " 'natural': 710,\n",
       " 'security': 711,\n",
       " 'sound': 712,\n",
       " 'develop': 713,\n",
       " 'jews': 714,\n",
       " 'chemical': 715,\n",
       " 'chance': 716,\n",
       " 'provide': 717,\n",
       " 'necessary': 718,\n",
       " 'reasons': 719,\n",
       " 'anti': 720,\n",
       " 'must': 721,\n",
       " 'board': 722,\n",
       " 'exams': 723,\n",
       " 'laptop': 724,\n",
       " 'grow': 725,\n",
       " '8': 726,\n",
       " 'likely': 727,\n",
       " 'sexual': 728,\n",
       " 'size': 729,\n",
       " 'center': 730,\n",
       " 'professional': 731,\n",
       " 'depression': 732,\n",
       " 'present': 733,\n",
       " 'remove': 734,\n",
       " 'add': 735,\n",
       " 'facts': 736,\n",
       " 'chances': 737,\n",
       " 'feeling': 738,\n",
       " 'army': 739,\n",
       " '11': 740,\n",
       " 'large': 741,\n",
       " 'reading': 742,\n",
       " 'accept': 743,\n",
       " 'within': 744,\n",
       " 'schools': 745,\n",
       " 'send': 746,\n",
       " 'cut': 747,\n",
       " 'apps': 748,\n",
       " 'dream': 749,\n",
       " 'option': 750,\n",
       " 'told': 751,\n",
       " 'speak': 752,\n",
       " 'together': 753,\n",
       " 'office': 754,\n",
       " 'prefer': 755,\n",
       " 'moving': 756,\n",
       " 'european': 757,\n",
       " 'boy': 758,\n",
       " 'husband': 759,\n",
       " 'poor': 760,\n",
       " 'd': 761,\n",
       " 'preparation': 762,\n",
       " 'becoming': 763,\n",
       " 'fake': 764,\n",
       " 'interested': 765,\n",
       " 'investment': 766,\n",
       " 'head': 767,\n",
       " 'fall': 768,\n",
       " 'least': 769,\n",
       " 'graduate': 770,\n",
       " 'digital': 771,\n",
       " 'break': 772,\n",
       " 'financial': 773,\n",
       " 'close': 774,\n",
       " 'obama': 775,\n",
       " 'nothing': 776,\n",
       " 'interesting': 777,\n",
       " 'paid': 778,\n",
       " 'income': 779,\n",
       " 'mental': 780,\n",
       " 'western': 781,\n",
       " 'thoughts': 782,\n",
       " 'here': 783,\n",
       " 'dating': 784,\n",
       " 'illegal': 785,\n",
       " 'options': 786,\n",
       " 'percentage': 787,\n",
       " 'impact': 788,\n",
       " 'nuclear': 789,\n",
       " 'outside': 790,\n",
       " 'contact': 791,\n",
       " 'subject': 792,\n",
       " 'drive': 793,\n",
       " 'basic': 794,\n",
       " 'french': 795,\n",
       " 'universities': 796,\n",
       " 'upsc': 797,\n",
       " '30': 798,\n",
       " 'languages': 799,\n",
       " 'gain': 800,\n",
       " 'allow': 801,\n",
       " 'comes': 802,\n",
       " 'higher': 803,\n",
       " 'store': 804,\n",
       " 'uses': 805,\n",
       " 'studies': 806,\n",
       " 'baby': 807,\n",
       " 'land': 808,\n",
       " 'population': 809,\n",
       " 'aren': 810,\n",
       " 'java': 811,\n",
       " 'style': 812,\n",
       " 'physical': 813,\n",
       " 'might': 814,\n",
       " 'rights': 815,\n",
       " 'born': 816,\n",
       " 'interest': 817,\n",
       " 'special': 818,\n",
       " 'hand': 819,\n",
       " 'teacher': 820,\n",
       " 'chemistry': 821,\n",
       " 'gate': 822,\n",
       " 'advantages': 823,\n",
       " 'solution': 824,\n",
       " 'mains': 825,\n",
       " 'characteristics': 826,\n",
       " 'personality': 827,\n",
       " 'ms': 828,\n",
       " 'websites': 829,\n",
       " 'check': 830,\n",
       " 'racist': 831,\n",
       " 'german': 832,\n",
       " 'changed': 833,\n",
       " 'issues': 834,\n",
       " 'intelligence': 835,\n",
       " 'son': 836,\n",
       " 'email': 837,\n",
       " 'fact': 838,\n",
       " 'heart': 839,\n",
       " 'developer': 840,\n",
       " 'gets': 841,\n",
       " 'doctor': 842,\n",
       " '18': 843,\n",
       " 'rich': 844,\n",
       " 'apple': 845,\n",
       " 'room': 846,\n",
       " 'created': 847,\n",
       " 'strong': 848,\n",
       " 'recommend': 849,\n",
       " 'meet': 850,\n",
       " 'west': 851,\n",
       " 'fat': 852,\n",
       " '9': 853,\n",
       " 'russian': 854,\n",
       " 'pressure': 855,\n",
       " 'please': 856,\n",
       " 'branch': 857,\n",
       " 'credit': 858,\n",
       " 'economy': 859,\n",
       " 'sites': 860,\n",
       " 'pregnant': 861,\n",
       " 'marry': 862,\n",
       " 'hindi': 863,\n",
       " 'startup': 864,\n",
       " 'dark': 865,\n",
       " 'laws': 866,\n",
       " 'currently': 867,\n",
       " 'positive': 868,\n",
       " 'pass': 869,\n",
       " 'structure': 870,\n",
       " 'charge': 871,\n",
       " 'source': 872,\n",
       " 'ones': 873,\n",
       " 'pursue': 874,\n",
       " 'near': 875,\n",
       " 'africa': 876,\n",
       " 'inside': 877,\n",
       " 'practice': 878,\n",
       " 'partner': 879,\n",
       " 'early': 880,\n",
       " 'solve': 881,\n",
       " 'hyderabad': 882,\n",
       " '50': 883,\n",
       " 'scope': 884,\n",
       " 'spend': 885,\n",
       " 'building': 886,\n",
       " 'smart': 887,\n",
       " 'network': 888,\n",
       " 'crush': 889,\n",
       " 'bitcoin': 890,\n",
       " 'talking': 891,\n",
       " 'bring': 892,\n",
       " 'christians': 893,\n",
       " 'ok': 894,\n",
       " 'views': 895,\n",
       " 'whole': 896,\n",
       " 'art': 897,\n",
       " 'videos': 898,\n",
       " 'mom': 899,\n",
       " 'completely': 900,\n",
       " 'numbers': 901,\n",
       " 'windows': 902,\n",
       " 'iq': 903,\n",
       " 'maths': 904,\n",
       " 'officer': 905,\n",
       " 'topics': 906,\n",
       " 'alone': 907,\n",
       " 'ca': 908,\n",
       " 'dogs': 909,\n",
       " 'film': 910,\n",
       " 'among': 911,\n",
       " 'certain': 912,\n",
       " 'pune': 913,\n",
       " 'cell': 914,\n",
       " 'healthy': 915,\n",
       " 'content': 916,\n",
       " 'global': 917,\n",
       " 'cbse': 918,\n",
       " 'christian': 919,\n",
       " 'especially': 920,\n",
       " 'gas': 921,\n",
       " 'works': 922,\n",
       " 'factors': 923,\n",
       " 'complete': 924,\n",
       " 'exactly': 925,\n",
       " 'yes': 926,\n",
       " 'according': 927,\n",
       " 'written': 928,\n",
       " 'needed': 929,\n",
       " 'tech': 930,\n",
       " 'fear': 931,\n",
       " 'distance': 932,\n",
       " 'asian': 933,\n",
       " 'stupid': 934,\n",
       " 'save': 935,\n",
       " 'religious': 936,\n",
       " 'text': 937,\n",
       " 'e': 938,\n",
       " 'reduce': 939,\n",
       " 'cold': 940,\n",
       " 'songs': 941,\n",
       " 'treatment': 942,\n",
       " 'ago': 943,\n",
       " 'east': 944,\n",
       " 'master': 945,\n",
       " 'electrical': 946,\n",
       " 'file': 947,\n",
       " 'playing': 948,\n",
       " 'coming': 949,\n",
       " 'race': 950,\n",
       " 'hurt': 951,\n",
       " 'insurance': 952,\n",
       " 'eyes': 953,\n",
       " 'giving': 954,\n",
       " 'attack': 955,\n",
       " 'buying': 956,\n",
       " 'community': 957,\n",
       " 'novel': 958,\n",
       " 'train': 959,\n",
       " 'teach': 960,\n",
       " 'democrats': 961,\n",
       " 'advanced': 962,\n",
       " 'minimum': 963,\n",
       " 'policy': 964,\n",
       " 'simple': 965,\n",
       " 'african': 966,\n",
       " 'letter': 967,\n",
       " 'opportunities': 968,\n",
       " 'fix': 969,\n",
       " 'dead': 970,\n",
       " 'cs': 971,\n",
       " 'multiple': 972,\n",
       " 'thought': 973,\n",
       " 'hindu': 974,\n",
       " 'sense': 975,\n",
       " 'running': 976,\n",
       " 'mathematics': 977,\n",
       " 'eye': 978,\n",
       " 'result': 979,\n",
       " 'procedure': 980,\n",
       " 'almost': 981,\n",
       " 'method': 982,\n",
       " 'easily': 983,\n",
       " 'abroad': 984,\n",
       " 'green': 985,\n",
       " 'boys': 986,\n",
       " 'anxiety': 987,\n",
       " 'transfer': 988,\n",
       " 'lead': 989,\n",
       " 'front': 990,\n",
       " 'football': 991,\n",
       " 'react': 992,\n",
       " 'clear': 993,\n",
       " 'lower': 994,\n",
       " 'half': 995,\n",
       " 'search': 996,\n",
       " 'economic': 997,\n",
       " 'importance': 998,\n",
       " 'cars': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfvec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1362492, 210366)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 300)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.128611443416291"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tfidfvec.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.376121887236601"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(tfidfvec.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = 'glove.840B.300d.txt'\n",
    "    \n",
    "    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "    \n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std = -0.005838499, 0.48782197\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    \n",
    "    # Why random embedding for OOV? what if use mean?\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    \n",
    "    #embedding_matrix = np.random.normal(emb_mean, 0, (nb_words, embed_size)) # std 0\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector * tfidfvec.idf_[i]\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "glove_embeddings = load_glove(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 300)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(glove_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([      0,       1,       2, ..., 1306116, 1306118, 1306119]),\n",
       "  array([      7,      11,      12, ..., 1306117, 1306120, 1306121])),\n",
       " (array([      0,       1,       3, ..., 1306118, 1306120, 1306121]),\n",
       "  array([      2,       4,       5, ..., 1306115, 1306116, 1306119])),\n",
       " (array([      2,       4,       5, ..., 1306119, 1306120, 1306121]),\n",
       "  array([      0,       1,       3, ..., 1306101, 1306103, 1306118]))]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = list(StratifiedKFold(n_splits=3, shuffle=True, random_state=1024).split(x_train, y_train))\n",
    "splits[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code inspired from: https://github.com/anandsaha/pytorch.cyclic.learning.rate/blob/master/cls.py\n",
    "class CyclicLR(object):\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "embedding_dim = 300\n",
    "embedding_path = '../save/embedding_matrix.npy'  # or False, not use pre-trained-matrix\n",
    "use_pretrained_embedding = True\n",
    "\n",
    "hidden_size = 60\n",
    "gru_len = hidden_size\n",
    "\n",
    "Routings = 4 #5\n",
    "Num_capsule = 5\n",
    "Dim_capsule = 5#16\n",
    "dropout_p = 0.25\n",
    "rate_drop_dense = 0.28\n",
    "LR = 0.001\n",
    "T_epsilon = 1e-7\n",
    "num_classes = 30\n",
    "\n",
    "\n",
    "class Embed_Layer(nn.Module):\n",
    "    def __init__(self, embedding_matrix=None, vocab_size=None, embedding_dim=300):\n",
    "        super(Embed_Layer, self).__init__()\n",
    "        self.encoder = nn.Embedding(vocab_size + 1, embedding_dim)\n",
    "        if use_pretrained_embedding:\n",
    "            self.encoder.weight.data.copy_(t.from_numpy(embedding_matrix)) \n",
    "\n",
    "    def forward(self, x, dropout_p=0.25):\n",
    "        return nn.Dropout(p=dropout_p)(self.encoder(x))\n",
    "\n",
    "\n",
    "class GRU_Layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRU_Layer, self).__init__()\n",
    "        self.gru = nn.GRU(input_size=300,\n",
    "                          hidden_size=gru_len,\n",
    "                          bidirectional=True)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for k in ih:\n",
    "            nn.init.xavier_uniform_(k)\n",
    "        for k in hh:\n",
    "            nn.init.orthogonal_(k)\n",
    "        for k in b:\n",
    "            nn.init.constant_(k, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gru(x)\n",
    "\n",
    "\n",
    "# core caps_layer with squash func\n",
    "class Caps_Layer(nn.Module):\n",
    "    def __init__(self, input_dim_capsule=gru_len * 2, num_capsule=Num_capsule, dim_capsule=Dim_capsule, \\\n",
    "                 routings=Routings, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Caps_Layer, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size \n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = self.squash\n",
    "        else:\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "        if self.share_weights:\n",
    "            self.W = nn.Parameter(\n",
    "                nn.init.xavier_normal_(t.empty(1, input_dim_capsule, self.num_capsule * self.dim_capsule)))\n",
    "        else:\n",
    "            self.W = nn.Parameter(\n",
    "                t.randn(BATCH_SIZE, input_dim_capsule, self.num_capsule * self.dim_capsule))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = t.matmul(x, self.W)\n",
    "        else:\n",
    "            print('add later')\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        input_num_capsule = x.size(1)\n",
    "        u_hat_vecs = u_hat_vecs.view((batch_size, input_num_capsule,\n",
    "                                      self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = u_hat_vecs.permute(0, 2, 1, 3)  \n",
    "        b = t.zeros_like(u_hat_vecs[:, :, :, 0])\n",
    "\n",
    "        for i in range(self.routings):\n",
    "            b = b.permute(0, 2, 1)\n",
    "            c = F.softmax(b, dim=2)\n",
    "            c = c.permute(0, 2, 1)\n",
    "            b = b.permute(0, 2, 1)\n",
    "            outputs = self.activation(t.einsum('bij,bijk->bik', (c, u_hat_vecs)))  # batch matrix multiplication\n",
    "            # outputs shape (batch_size, num_capsule, dim_capsule)\n",
    "            if i < self.routings - 1:\n",
    "                b = t.einsum('bik,bijk->bij', (outputs, u_hat_vecs))  # batch matrix multiplication\n",
    "        return outputs  # (batch_size, num_capsule, dim_capsule)\n",
    "\n",
    "    # text version of squash, slight different from original one\n",
    "    def squash(self, x, axis=-1):\n",
    "        s_squared_norm = (x ** 2).sum(axis, keepdim=True)\n",
    "        scale = t.sqrt(s_squared_norm + T_epsilon)\n",
    "        return x / scale\n",
    "    \n",
    "class Capsule_Main(nn.Module):\n",
    "    def __init__(self, embedding_matrix=None, vocab_size=None):\n",
    "        super(Capsule_Main, self).__init__()\n",
    "        self.embed_layer = Embed_Layer(embedding_matrix, vocab_size)\n",
    "        self.gru_layer = GRU_Layer()\n",
    "        self.gru_layer.init_weights()\n",
    "        self.caps_layer = Caps_Layer()\n",
    "        self.dense_layer = Dense_Layer()\n",
    "\n",
    "    def forward(self, content):\n",
    "        content1 = self.embed_layer(content)\n",
    "        content2, _ = self.gru_layer(content1)  \n",
    "        content3 = self.caps_layer(content2)\n",
    "        output = self.dense_layer(content3)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        fc_layer = 16\n",
    "        fc_layer1 = 16\n",
    "\n",
    "        self.embedding = nn.Embedding(max_features, 300)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(glove_embeddings, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        self.lstm = nn.LSTM(300, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.lstm_attention = Attention(hidden_size * 2, max_len)\n",
    "        self.gru_attention = Attention(hidden_size * 2, max_len)\n",
    "        self.bn = nn.BatchNorm1d(16, momentum=0.5)\n",
    "        self.linear = nn.Linear(hidden_size*8+3, fc_layer1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(fc_layer**2,fc_layer)\n",
    "        self.out = nn.Linear(fc_layer, 1)\n",
    "        self.lincaps = nn.Linear(Num_capsule * Dim_capsule, 1)\n",
    "        self.caps_layer = Caps_Layer()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Capsule(num_capsule=10, dim_capsule=10, routings=4, share_weights=True)(x)\n",
    "        h_embedding = self.embedding(x[0])\n",
    "        h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n",
    "        \n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        h_gru, _ = self.gru(h_lstm)\n",
    "\n",
    "        ##Capsule Layer        \n",
    "        content3 = self.caps_layer(h_gru)\n",
    "        content3 = self.dropout(content3)\n",
    "        batch_size = content3.size(0)\n",
    "        content3 = content3.view(batch_size, -1)\n",
    "        content3 = self.relu(self.lincaps(content3))\n",
    "\n",
    "        ##Attention Layer\n",
    "        h_lstm_atten = self.lstm_attention(h_lstm)\n",
    "        h_gru_atten = self.gru_attention(h_gru)\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_gru, 1)\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_gru, 1)\n",
    "        \n",
    "        f = torch.tensor(x[1], dtype=torch.float).cuda()\n",
    "\n",
    "                #[512,160]\n",
    "        conc = torch.cat((h_lstm_atten, h_gru_atten,content3, avg_pool, max_pool,f), 1)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.bn(conc)\n",
    "        conc = self.dropout(conc)\n",
    "\n",
    "        out = self.out(conc)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.dataset[index]\n",
    "        return data, target, index\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sradheya/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 \t loss=8.7073 \t val_loss=7.0456 \t time=242.86s\n",
      "Epoch 2/25 \t loss=7.8122 \t val_loss=7.0161 \t time=242.86s\n",
      "Epoch 3/25 \t loss=7.5623 \t val_loss=6.7607 \t time=242.74s\n",
      "Epoch 4/25 \t loss=7.3845 \t val_loss=6.8907 \t time=242.11s\n",
      "Epoch 5/25 \t loss=7.2380 \t val_loss=6.6674 \t time=242.40s\n",
      "Epoch 6/25 \t loss=7.1599 \t val_loss=6.6634 \t time=242.61s\n",
      "Epoch 7/25 \t loss=7.0691 \t val_loss=6.7564 \t time=242.69s\n",
      "Epoch 8/25 \t loss=6.9896 \t val_loss=6.8053 \t time=242.59s\n",
      "Epoch 9/25 \t loss=6.9157 \t val_loss=6.9547 \t time=242.77s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-1796a328c35f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfold_X_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0;31m################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-69adcb5e9c28>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mh_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mh_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mh_gru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_lstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "train_preds = np.zeros((len(x_train)))\n",
    "test_preds = np.zeros((len(x_test)))\n",
    "\n",
    "# always call this before training for deterministic results\n",
    "seed_everything()\n",
    "\n",
    "x_test_cuda = torch.tensor(x_test, dtype=torch.long).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=64, shuffle=False)\n",
    "\n",
    "avg_losses_f = []\n",
    "avg_val_losses_f = []\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):    \n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    features = np.array(features)\n",
    "\n",
    "    x_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.long).cuda()\n",
    "    y_train_fold = torch.tensor(y_train[train_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    kfold_X_features = features[train_idx.astype(int)]\n",
    "    kfold_X_valid_features = features[valid_idx.astype(int)]\n",
    "    x_val_fold = torch.tensor(x_train[valid_idx.astype(int)], dtype=torch.long).cuda()\n",
    "    y_val_fold = torch.tensor(y_train[valid_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    # model = BiLSTM(lstm_layer=2,hidden_dim=40,dropout=DROPOUT).cuda()\n",
    "    model = NeuralNet()\n",
    "\n",
    "    # make sure everything in the model is running on the GPU\n",
    "    model.cuda()\n",
    "\n",
    "    # define binary cross entropy loss\n",
    "    # note that the model returns logit to take advantage of the log-sum-exp trick \n",
    "    # for numerical stability in the loss\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "    step_size = 300\n",
    "    base_lr, max_lr = 0.001, 0.003   \n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=max_lr)\n",
    "\n",
    "    scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, step_size=step_size, mode='exp_range', gamma=0.99994)\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    \n",
    "    train = MyDataset(train)\n",
    "    valid = MyDataset(valid)\n",
    "\n",
    "    ##No need to shuffle the data again here. Shuffling happens when splitting for kfolds.\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=64, shuffle=False)\n",
    "\n",
    "    print(f'Fold {i + 1}')\n",
    "    for epoch in range(25):\n",
    "        # set train mode of the model. This enables operations which are only applied during training like dropout\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "\n",
    "        avg_loss = 0.  \n",
    "        for i, (x_batch, y_batch, index) in enumerate(train_loader):\n",
    "            # Forward pass: compute predicted y by passing x to the model.\n",
    "            ################################################################################################            \n",
    "            f = kfold_X_features[index]\n",
    "            y_pred = model([x_batch, f])\n",
    "            ################################################################################################\n",
    "\n",
    "            ################################################################################################\n",
    "            if scheduler:\n",
    "                scheduler.batch_step()\n",
    "            ################################################################################################\n",
    "\n",
    "\n",
    "            # Compute and print loss.\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            # Before the backward pass, use the optimizer object to zero all of the\n",
    "            # gradients for the Tensors it will update (which are the learnable weights\n",
    "            # of the model)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Calling the step function on an Optimizer makes an update to its parameters\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            \n",
    "        # set evaluation mode of the model. This disabled operations which are only applied during training like dropout\n",
    "        model.eval()\n",
    "        \n",
    "        # predict all the samples in y_val_fold batch per batch\n",
    "        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "        test_preds_fold = np.zeros((len(x_test)))\n",
    "        \n",
    "        avg_val_loss = 0.\n",
    "        for i, (x_batch, y_batch, index) in enumerate(valid_loader):\n",
    "            f = kfold_X_valid_features[index]\n",
    "            y_pred = model([x_batch, f]).detach()\n",
    "            \n",
    "            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "            valid_preds_fold[i * 64:(i+1) * 64] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "        \n",
    "        elapsed_time = time.time() - start_time \n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n",
    "            epoch + 1, 25, avg_loss, avg_val_loss, elapsed_time))\n",
    "    avg_losses_f.append(avg_loss)\n",
    "    avg_val_losses_f.append(avg_val_loss) \n",
    "    # predict all samples in the test set batch per batch\n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        f = test_features[i * 64:(i+1) * 64]\n",
    "        y_pred = model([x_batch, f]).detach()\n",
    "\n",
    "        test_preds_fold[i * 64:(i+1) * 64] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "        \n",
    "    train_preds[valid_idx] = valid_preds_fold\n",
    "    test_preds += test_preds_fold / len(splits)\n",
    "\n",
    "print('All \\t loss={:.4f} \\t val_loss={:.4f} \\t '.format(np.average(avg_losses_f),np.average(avg_val_losses_f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/41 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/41 [00:00<00:06,  5.94it/s]\u001b[A\n",
      "  5%|▍         | 2/41 [00:00<00:06,  5.97it/s]\u001b[A\n",
      "  7%|▋         | 3/41 [00:00<00:06,  6.01it/s]\u001b[A\n",
      " 10%|▉         | 4/41 [00:00<00:06,  6.06it/s]\u001b[A\n",
      " 12%|█▏        | 5/41 [00:00<00:05,  6.12it/s]\u001b[A\n",
      " 15%|█▍        | 6/41 [00:00<00:05,  6.16it/s]\u001b[A\n",
      " 17%|█▋        | 7/41 [00:01<00:05,  6.15it/s]\u001b[A\n",
      " 20%|█▉        | 8/41 [00:01<00:05,  6.01it/s]\u001b[A\n",
      " 22%|██▏       | 9/41 [00:01<00:05,  5.98it/s]\u001b[A\n",
      " 24%|██▍       | 10/41 [00:01<00:05,  6.04it/s]\u001b[A\n",
      " 27%|██▋       | 11/41 [00:01<00:04,  6.07it/s]\u001b[A\n",
      " 29%|██▉       | 12/41 [00:01<00:04,  6.12it/s]\u001b[A\n",
      " 32%|███▏      | 13/41 [00:02<00:04,  6.10it/s]\u001b[A\n",
      " 34%|███▍      | 14/41 [00:02<00:04,  6.14it/s]\u001b[A\n",
      " 37%|███▋      | 15/41 [00:02<00:04,  6.18it/s]\u001b[A\n",
      " 39%|███▉      | 16/41 [00:02<00:04,  6.18it/s]\u001b[A\n",
      " 41%|████▏     | 17/41 [00:02<00:03,  6.19it/s]\u001b[A\n",
      " 44%|████▍     | 18/41 [00:02<00:03,  6.25it/s]\u001b[A\n",
      " 46%|████▋     | 19/41 [00:03<00:03,  6.28it/s]\u001b[A\n",
      " 49%|████▉     | 20/41 [00:03<00:03,  6.31it/s]\u001b[A\n",
      " 51%|█████     | 21/41 [00:03<00:03,  6.33it/s]\u001b[A\n",
      " 54%|█████▎    | 22/41 [00:03<00:03,  6.33it/s]\u001b[A\n",
      " 56%|█████▌    | 23/41 [00:03<00:02,  6.32it/s]\u001b[A\n",
      " 59%|█████▊    | 24/41 [00:03<00:02,  6.33it/s]\u001b[A\n",
      " 61%|██████    | 25/41 [00:04<00:02,  6.34it/s]\u001b[A\n",
      " 63%|██████▎   | 26/41 [00:04<00:02,  6.36it/s]\u001b[A\n",
      " 66%|██████▌   | 27/41 [00:04<00:02,  6.37it/s]\u001b[A\n",
      " 68%|██████▊   | 28/41 [00:04<00:02,  6.39it/s]\u001b[A\n",
      " 71%|███████   | 29/41 [00:04<00:01,  6.34it/s]\u001b[A\n",
      " 73%|███████▎  | 30/41 [00:04<00:01,  6.37it/s]\u001b[A\n",
      " 76%|███████▌  | 31/41 [00:04<00:01,  6.38it/s]\u001b[A\n",
      " 78%|███████▊  | 32/41 [00:05<00:01,  6.37it/s]\u001b[A\n",
      " 80%|████████  | 33/41 [00:05<00:01,  6.38it/s]\u001b[A\n",
      " 83%|████████▎ | 34/41 [00:05<00:01,  6.39it/s]\u001b[A\n",
      " 85%|████████▌ | 35/41 [00:05<00:00,  6.40it/s]\u001b[A\n",
      " 88%|████████▊ | 36/41 [00:05<00:00,  6.41it/s]\u001b[A\n",
      " 90%|█████████ | 37/41 [00:05<00:00,  6.41it/s]\u001b[A\n",
      " 93%|█████████▎| 38/41 [00:06<00:00,  6.42it/s]\u001b[A\n",
      " 95%|█████████▌| 39/41 [00:06<00:00,  6.43it/s]\u001b[A\n",
      " 98%|█████████▊| 40/41 [00:06<00:00,  6.41it/s]\u001b[A\n",
      "100%|██████████| 41/41 [00:06<00:00,  6.31it/s]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold is 0.2700 with F1 score: 0.6683\n"
     ]
    }
   ],
   "source": [
    "def bestThresshold(y_train,train_preds):\n",
    "    tmp = [0,0,0] # idx, cur, max\n",
    "    delta = 0\n",
    "    for tmp[0] in tqdm(np.arange(0.1, 0.501, 0.01)):\n",
    "        tmp[1] = f1_score(y_train, np.array(train_preds)>tmp[0])\n",
    "        if tmp[1] > tmp[2]:\n",
    "            delta = tmp[0]\n",
    "            tmp[2] = tmp[1]\n",
    "    print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))\n",
    "    return delta\n",
    "delta = bestThresshold(y_train,train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = df_test[['qid']].copy()\n",
    "submission['prediction'] = (test_preds > delta).astype(int)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
