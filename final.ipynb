{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Required Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "import torch\n",
    "from torchtext import data\n",
    "import spacy\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas(desc='Progress')\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from torchtext.data import Example\n",
    "from sklearn.metrics import f1_score\n",
    "import torchtext\n",
    "import os \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# cross validation and metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1024):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (56370, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "print(\"Train shape : \",train.shape)\n",
    "print(\"Test shape : \",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1362492, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = pd.concat([train, test])\n",
    "total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose =  True):\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1362492/1362492 [00:03<00:00, 352799.70it/s]\n",
      "100%|██████████| 1362492/1362492 [00:03<00:00, 440624.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'How': 273144, 'did': 34918, 'Quebec': 102, 'nationalists': 97, 'see': 9397}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = total[\"question_text\"].progress_apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)\n",
    "print({k: vocab[k] for k in list(vocab)[:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 120000\n",
    "max_len = 72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOOGLE NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "news_path = 'GoogleNews-vectors-negative300.bin'\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522569/522569 [00:00<00:00, 547433.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 24.05% of vocab\n",
      "Found embeddings for  78.75% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 420476),\n",
       " ('a', 419837),\n",
       " ('of', 345145),\n",
       " ('and', 262815),\n",
       " ('India?', 17082),\n",
       " ('it?', 13436),\n",
       " ('do?', 9112),\n",
       " ('life?', 8074),\n",
       " ('you?', 6553),\n",
       " ('me?', 6485)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'?' in embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'&' in embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = 'glove.840B.300d.txt'\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(new_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522569/522569 [00:00<00:00, 942041.56it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 32.91% of vocab\n",
      "Found embeddings for  88.16% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('India?', 17082),\n",
       " ('it?', 13436),\n",
       " (\"What's\", 12985),\n",
       " ('do?', 9112),\n",
       " ('life?', 8074),\n",
       " ('you?', 6553),\n",
       " ('me?', 6485),\n",
       " ('them?', 6421),\n",
       " ('time?', 5994),\n",
       " ('world?', 5632)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'?' in embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'&' in embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAST TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = 'wiki-news-300d-1M.vec'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(new_path) if len(o)>100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522569/522569 [00:00<00:00, 975716.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 29.77% of vocab\n",
      "Found embeddings for  87.66% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('India?', 17082),\n",
       " (\"don't\", 15642),\n",
       " ('it?', 13436),\n",
       " (\"I'm\", 13344),\n",
       " (\"What's\", 12985),\n",
       " ('do?', 9112),\n",
       " ('life?', 8074),\n",
       " (\"can't\", 7375),\n",
       " ('you?', 6553),\n",
       " ('me?', 6485)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'?' in embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'&' in embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = 'paragram_300_sl999.txt'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(new_path, encoding=\"utf8\", errors='ignore') if len(o)>100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522569/522569 [00:00<00:00, 977781.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 19.42% of vocab\n",
      "Found embeddings for  72.21% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What', 436013),\n",
       " ('I', 319441),\n",
       " ('How', 273144),\n",
       " ('Why', 148582),\n",
       " ('Is', 113627),\n",
       " ('Can', 54992),\n",
       " ('Which', 49357),\n",
       " ('Do', 41756),\n",
       " ('If', 35896),\n",
       " ('Are', 30442)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1362492/1362492 [00:07<00:00, 176414.59it/s]\n",
      "100%|██████████| 1362492/1362492 [00:02<00:00, 493915.93it/s]\n"
     ]
    }
   ],
   "source": [
    "total[\"question_text\"] = total[\"question_text\"].progress_apply(lambda x: clean_text(x))\n",
    "sentences = total[\"question_text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = 'glove.840B.300d.txt'\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(new_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 259819/259819 [00:00<00:00, 916039.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 70.96% of vocab\n",
      "Found embeddings for  99.44% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Quorans', 885),\n",
       " ('Brexit', 510),\n",
       " ('cryptocurrencies', 506),\n",
       " ('Redmi', 394),\n",
       " ('OnePlus', 130),\n",
       " ('UCEED', 126),\n",
       " ('GDPR', 110),\n",
       " ('Blockchain', 110),\n",
       " ('demonetisation', 109),\n",
       " ('Coinbase', 105),\n",
       " ('BNBR', 104),\n",
       " ('Machedo', 103),\n",
       " ('Adityanath', 101),\n",
       " ('Boruto', 96),\n",
       " ('ethereum', 94),\n",
       " ('DCEU', 93),\n",
       " ('IIEST', 90),\n",
       " ('SJWs', 86),\n",
       " ('Qoura', 81),\n",
       " ('LNMIIT', 72)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mispell_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1362492/1362492 [00:25<00:00, 53723.31it/s]\n",
      "Progress: 100%|██████████| 1362492/1362492 [00:03<00:00, 423386.95it/s]\n",
      "100%|██████████| 1362492/1362492 [00:02<00:00, 508714.25it/s]\n"
     ]
    }
   ],
   "source": [
    "total[\"question_text\"] = total[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "sentences = total[\"question_text\"].progress_apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 259726/259726 [00:00<00:00, 974089.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 70.96% of vocab\n",
      "Found embeddings for  99.44% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def add_features(df):\n",
    "    \n",
    "    df['question_text'] = df['question_text'].progress_apply(lambda x:str(x))\n",
    "    df['total_length'] = df['question_text'].progress_apply(len)\n",
    "    df['capitals'] = df['question_text'].progress_apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.progress_apply(lambda row: float(row['capitals'])/(float(row['total_length']) + 0.0000001),\n",
    "                                axis=1)\n",
    "    df['num_words'] = df.question_text.str.count('\\S+')\n",
    "    df['num_unique_words'] = df['question_text'].progress_apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / (df['num_words'] + 0.0000001)\n",
    "    return df\n",
    "\n",
    "def load_and_prec():\n",
    "    \n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    print(\"Train shape : \",train_df.shape)\n",
    "    print(\"Test shape : \",test_df.shape)\n",
    "    \n",
    "    # lower\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "\n",
    "    # Clean the text\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_text(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "    \n",
    "    # Clean speelings\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "    \n",
    "    ## fill up the missing values\n",
    "    train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n",
    "    test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n",
    "    \n",
    "    train = add_features(train_df)\n",
    "    test = add_features(test_df)\n",
    "\n",
    "    features = train[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "    test_features = test[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(np.vstack((features, test_features)))\n",
    "    features = ss.transform(features)\n",
    "    test_features = ss.transform(test_features)\n",
    "\n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words = max_features)\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=max_len)\n",
    "    test_X = pad_sequences(test_X, maxlen=max_len)\n",
    "\n",
    "    ## Get the target values\n",
    "    train_y = train_df['target'].values \n",
    "    \n",
    "    #shuffling the data\n",
    "    np.random.seed(1024)\n",
    "    trn_idx = np.random.permutation(len(train_X))\n",
    "\n",
    "    train_X = train_X[trn_idx]\n",
    "    train_y = train_y[trn_idx]\n",
    "    \n",
    "    return train_X, test_X, train_y, features, test_features, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (56370, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress:   0%|          | 0/1306122 [00:00<?, ?it/s]\u001b[A\n",
      "Progress:   1%|          | 14346/1306122 [00:00<00:09, 143457.06it/s]\u001b[A\n",
      "Progress:   2%|▏         | 30733/1306122 [00:00<00:08, 149025.68it/s]\u001b[A\n",
      "Progress:   4%|▎         | 47806/1306122 [00:00<00:08, 154933.00it/s]\u001b[A\n",
      "Progress:   5%|▍         | 65268/1306122 [00:00<00:07, 160355.67it/s]\u001b[A\n",
      "Progress:   6%|▋         | 82716/1306122 [00:00<00:07, 164346.37it/s]\u001b[A\n",
      "Progress:   8%|▊         | 100270/1306122 [00:00<00:07, 167551.23it/s]\u001b[A\n",
      "Progress:   9%|▉         | 117600/1306122 [00:00<00:07, 169235.22it/s]\u001b[A\n",
      "Progress:  10%|█         | 134921/1306122 [00:00<00:06, 170406.50it/s]\u001b[A\n",
      "Progress:  12%|█▏        | 152596/1306122 [00:00<00:06, 172258.14it/s]\u001b[A\n",
      "Progress:  13%|█▎        | 170269/1306122 [00:01<00:06, 173574.71it/s]\u001b[A\n",
      "Progress:  14%|█▍        | 187715/1306122 [00:01<00:06, 173839.03it/s]\u001b[A\n",
      "Progress:  16%|█▌        | 204979/1306122 [00:01<00:06, 173475.38it/s]\u001b[A\n",
      "Progress:  17%|█▋        | 222143/1306122 [00:01<00:06, 172571.03it/s]\u001b[A\n",
      "Progress:  18%|█▊        | 239505/1306122 [00:01<00:06, 172882.46it/s]\u001b[A\n",
      "Progress:  20%|█▉        | 256877/1306122 [00:01<00:06, 173132.68it/s]\u001b[A\n",
      "Progress:  21%|██        | 274548/1306122 [00:01<00:05, 174190.50it/s]\u001b[A\n",
      "Progress:  22%|██▏       | 292227/1306122 [00:01<00:05, 174959.98it/s]\u001b[A\n",
      "Progress:  24%|██▎       | 309937/1306122 [00:01<00:05, 175594.72it/s]\u001b[A\n",
      "Progress:  25%|██▌       | 327538/1306122 [00:01<00:05, 175718.65it/s]\u001b[A\n",
      "Progress:  26%|██▋       | 345097/1306122 [00:02<00:05, 175656.45it/s]\u001b[A\n",
      "Progress:  28%|██▊       | 362714/1306122 [00:02<00:05, 175808.49it/s]\u001b[A\n",
      "Progress:  29%|██▉       | 380289/1306122 [00:02<00:05, 175787.22it/s]\u001b[A\n",
      "Progress:  30%|███       | 397864/1306122 [00:02<00:05, 172535.93it/s]\u001b[A\n",
      "Progress:  32%|███▏      | 415560/1306122 [00:02<00:05, 173837.23it/s]\u001b[A\n",
      "Progress:  33%|███▎      | 432954/1306122 [00:02<00:05, 173516.52it/s]\u001b[A\n",
      "Progress:  34%|███▍      | 450313/1306122 [00:02<00:04, 173278.91it/s]\u001b[A\n",
      "Progress:  36%|███▌      | 467733/1306122 [00:02<00:04, 173553.15it/s]\u001b[A\n",
      "Progress:  37%|███▋      | 485196/1306122 [00:02<00:04, 173874.49it/s]\u001b[A\n",
      "Progress:  38%|███▊      | 502634/1306122 [00:02<00:04, 174022.28it/s]\u001b[A\n",
      "Progress:  40%|███▉      | 520039/1306122 [00:03<00:04, 173828.63it/s]\u001b[A\n",
      "Progress:  41%|████      | 537424/1306122 [00:03<00:04, 169660.03it/s]\u001b[A\n",
      "Progress:  42%|████▏     | 554414/1306122 [00:03<00:04, 167511.53it/s]\u001b[A\n",
      "Progress:  44%|████▎     | 571339/1306122 [00:03<00:04, 168027.45it/s]\u001b[A\n",
      "Progress:  45%|████▌     | 588158/1306122 [00:03<00:04, 167192.02it/s]\u001b[A\n",
      "Progress:  46%|████▋     | 605026/1306122 [00:03<00:04, 167635.10it/s]\u001b[A\n",
      "Progress:  48%|████▊     | 622235/1306122 [00:03<00:04, 168945.43it/s]\u001b[A\n",
      "Progress:  49%|████▉     | 639139/1306122 [00:03<00:03, 168005.55it/s]\u001b[A\n",
      "Progress:  50%|█████     | 655947/1306122 [00:03<00:03, 167896.71it/s]\u001b[A\n",
      "Progress:  52%|█████▏    | 672742/1306122 [00:03<00:03, 167557.12it/s]\u001b[A\n",
      "Progress:  53%|█████▎    | 689502/1306122 [00:04<00:03, 158036.86it/s]\u001b[A\n",
      "Progress:  54%|█████▍    | 706216/1306122 [00:04<00:03, 160660.01it/s]\u001b[A\n",
      "Progress:  55%|█████▌    | 723242/1306122 [00:04<00:03, 163422.63it/s]\u001b[A\n",
      "Progress:  57%|█████▋    | 740124/1306122 [00:04<00:03, 165003.22it/s]\u001b[A\n",
      "Progress:  58%|█████▊    | 757468/1306122 [00:04<00:03, 167446.15it/s]\u001b[A\n",
      "Progress:  59%|█████▉    | 774261/1306122 [00:04<00:03, 165930.13it/s]\u001b[A\n",
      "Progress:  61%|██████    | 790891/1306122 [00:04<00:03, 161117.97it/s]\u001b[A\n",
      "Progress:  62%|██████▏   | 807059/1306122 [00:04<00:03, 157343.30it/s]\u001b[A\n",
      "Progress:  63%|██████▎   | 822853/1306122 [00:04<00:03, 156671.25it/s]\u001b[A\n",
      "Progress:  64%|██████▍   | 839039/1306122 [00:04<00:02, 158191.10it/s]\u001b[A\n",
      "Progress:  65%|██████▌   | 854891/1306122 [00:05<00:02, 155008.03it/s]\u001b[A\n",
      "Progress:  67%|██████▋   | 871983/1306122 [00:05<00:02, 159459.51it/s]\u001b[A\n",
      "Progress:  68%|██████▊   | 888484/1306122 [00:05<00:02, 161083.19it/s]\u001b[A\n",
      "Progress:  69%|██████▉   | 904637/1306122 [00:05<00:02, 159883.00it/s]\u001b[A\n",
      "Progress:  71%|███████   | 921668/1306122 [00:05<00:02, 162873.57it/s]\u001b[A\n",
      "Progress:  72%|███████▏  | 938156/1306122 [00:05<00:02, 163466.81it/s]\u001b[A\n",
      "Progress:  73%|███████▎  | 954530/1306122 [00:05<00:02, 163394.51it/s]\u001b[A\n",
      "Progress:  74%|███████▍  | 971524/1306122 [00:05<00:02, 165303.95it/s]\u001b[A\n",
      "Progress:  76%|███████▌  | 988678/1306122 [00:05<00:01, 167124.24it/s]\u001b[A\n",
      "Progress:  77%|███████▋  | 1005810/1306122 [00:05<00:01, 168359.26it/s]\u001b[A\n",
      "Progress:  78%|███████▊  | 1023188/1306122 [00:06<00:01, 169947.78it/s]\u001b[A\n",
      "Progress:  80%|███████▉  | 1040663/1306122 [00:06<00:01, 171360.43it/s]\u001b[A\n",
      "Progress:  81%|████████  | 1057812/1306122 [00:06<00:01, 170598.08it/s]\u001b[A\n",
      "Progress:  82%|████████▏ | 1075249/1306122 [00:06<00:01, 171711.37it/s]\u001b[A\n",
      "Progress:  84%|████████▎ | 1092429/1306122 [00:06<00:01, 171237.00it/s]\u001b[A\n",
      "Progress:  85%|████████▍ | 1109559/1306122 [00:06<00:01, 166195.66it/s]\u001b[A\n",
      "Progress:  86%|████████▌ | 1126216/1306122 [00:06<00:01, 164404.79it/s]\u001b[A\n",
      "Progress:  87%|████████▋ | 1142720/1306122 [00:06<00:00, 164592.07it/s]\u001b[A\n",
      "Progress:  89%|████████▉ | 1159201/1306122 [00:06<00:00, 163082.77it/s]\u001b[A\n",
      "Progress:  90%|█████████ | 1176398/1306122 [00:07<00:00, 165650.22it/s]\u001b[A\n",
      "Progress:  91%|█████████▏| 1193787/1306122 [00:07<00:00, 168038.79it/s]\u001b[A\n",
      "Progress:  93%|█████████▎| 1211215/1306122 [00:07<00:00, 169861.39it/s]\u001b[A\n",
      "Progress:  94%|█████████▍| 1228588/1306122 [00:07<00:00, 171003.45it/s]\u001b[A\n",
      "Progress:  95%|█████████▌| 1245905/1306122 [00:07<00:00, 171647.63it/s]\u001b[A\n",
      "Progress:  97%|█████████▋| 1263431/1306122 [00:07<00:00, 172714.44it/s]\u001b[A\n",
      "Progress:  98%|█████████▊| 1280792/1306122 [00:07<00:00, 172980.09it/s]\u001b[A\n",
      "Progress:  99%|█████████▉| 1298268/1306122 [00:07<00:00, 173509.43it/s]\u001b[A\n",
      "Progress: 100%|██████████| 1306122/1306122 [00:07<00:00, 168372.00it/s]\u001b[A\n",
      "Progress:   0%|          | 0/1306122 [00:00<?, ?it/s]\u001b[A\n",
      "Progress:   0%|          | 3939/1306122 [00:00<00:33, 39382.24it/s]\u001b[A\n",
      "Progress:   1%|          | 8738/1306122 [00:00<00:31, 41618.35it/s]\u001b[A\n",
      "Progress:   1%|          | 13745/1306122 [00:00<00:29, 43837.55it/s]\u001b[A\n",
      "Progress:   1%|▏         | 18669/1306122 [00:00<00:28, 45327.44it/s]\u001b[A\n",
      "Progress:   2%|▏         | 23684/1306122 [00:00<00:27, 46672.60it/s]\u001b[A\n",
      "Progress:   2%|▏         | 28783/1306122 [00:00<00:26, 47887.68it/s]\u001b[A\n",
      "Progress:   3%|▎         | 33893/1306122 [00:00<00:26, 48807.10it/s]\u001b[A\n",
      "Progress:   3%|▎         | 38955/1306122 [00:00<00:25, 49335.79it/s]\u001b[A\n",
      "Progress:   3%|▎         | 44110/1306122 [00:00<00:25, 49977.31it/s]\u001b[A\n",
      "Progress:   4%|▍         | 49273/1306122 [00:01<00:24, 50459.55it/s]\u001b[A\n",
      "Progress:   4%|▍         | 54322/1306122 [00:01<00:24, 50467.72it/s]\u001b[A\n",
      "Progress:   5%|▍         | 59474/1306122 [00:01<00:24, 50778.59it/s]\u001b[A\n",
      "Progress:   5%|▍         | 64581/1306122 [00:01<00:24, 50862.20it/s]\u001b[A\n",
      "Progress:   5%|▌         | 69635/1306122 [00:01<00:24, 50720.59it/s]\u001b[A\n",
      "Progress:   6%|▌         | 74735/1306122 [00:01<00:24, 50800.42it/s]\u001b[A\n",
      "Progress:   6%|▌         | 79800/1306122 [00:01<00:24, 50716.90it/s]\u001b[A\n",
      "Progress:   6%|▋         | 84882/1306122 [00:01<00:24, 50744.65it/s]\u001b[A\n",
      "Progress:   7%|▋         | 89949/1306122 [00:01<00:24, 50501.53it/s]\u001b[A\n",
      "Progress:   7%|▋         | 94996/1306122 [00:01<00:23, 50491.83it/s]\u001b[A\n",
      "Progress:   8%|▊         | 100050/1306122 [00:02<00:23, 50505.42it/s]\u001b[A\n",
      "Progress:   8%|▊         | 105098/1306122 [00:02<00:23, 50297.64it/s]\u001b[A\n",
      "Progress:   8%|▊         | 110127/1306122 [00:02<00:24, 49586.41it/s]\u001b[A\n",
      "Progress:   9%|▉         | 115087/1306122 [00:02<00:24, 49078.92it/s]\u001b[A\n",
      "Progress:   9%|▉         | 120002/1306122 [00:02<00:24, 49099.99it/s]\u001b[A\n",
      "Progress:  10%|▉         | 125008/1306122 [00:02<00:23, 49380.84it/s]\u001b[A\n",
      "Progress:  10%|▉         | 130155/1306122 [00:02<00:23, 49988.15it/s]\u001b[A\n",
      "Progress:  10%|█         | 135157/1306122 [00:02<00:23, 49865.63it/s]\u001b[A\n",
      "Progress:  11%|█         | 140146/1306122 [00:02<00:23, 49872.67it/s]\u001b[A\n",
      "Progress:  11%|█         | 145135/1306122 [00:02<00:23, 49657.44it/s]\u001b[A\n",
      "Progress:  11%|█▏        | 150204/1306122 [00:03<00:23, 49960.84it/s]\u001b[A\n",
      "Progress:  12%|█▏        | 155202/1306122 [00:03<00:23, 49950.21it/s]\u001b[A\n",
      "Progress:  12%|█▏        | 160227/1306122 [00:03<00:22, 50037.97it/s]\u001b[A\n",
      "Progress:  13%|█▎        | 165232/1306122 [00:03<00:22, 49993.65it/s]\u001b[A\n",
      "Progress:  13%|█▎        | 170232/1306122 [00:03<00:22, 49961.01it/s]\u001b[A\n",
      "Progress:  13%|█▎        | 175322/1306122 [00:03<00:22, 50237.68it/s]\u001b[A\n",
      "Progress:  14%|█▍        | 180347/1306122 [00:03<00:22, 50086.05it/s]\u001b[A\n",
      "Progress:  14%|█▍        | 185403/1306122 [00:03<00:22, 50226.71it/s]\u001b[A\n",
      "Progress:  15%|█▍        | 190427/1306122 [00:03<00:22, 50090.86it/s]\u001b[A\n",
      "Progress:  15%|█▍        | 195448/1306122 [00:03<00:22, 50124.77it/s]\u001b[A\n",
      "Progress:  15%|█▌        | 200461/1306122 [00:04<00:22, 49597.53it/s]\u001b[A\n",
      "Progress:  16%|█▌        | 205439/1306122 [00:04<00:22, 49651.44it/s]\u001b[A\n",
      "Progress:  16%|█▌        | 210419/1306122 [00:04<00:22, 49691.14it/s]\u001b[A\n",
      "Progress:  16%|█▋        | 215434/1306122 [00:04<00:21, 49826.48it/s]\u001b[A\n",
      "Progress:  17%|█▋        | 220418/1306122 [00:04<00:21, 49772.24it/s]\u001b[A\n",
      "Progress:  17%|█▋        | 225583/1306122 [00:04<00:21, 50320.92it/s]\u001b[A\n",
      "Progress:  18%|█▊        | 230645/1306122 [00:04<00:21, 50408.70it/s]\u001b[A\n",
      "Progress:  18%|█▊        | 235727/1306122 [00:04<00:21, 50531.23it/s]\u001b[A\n",
      "Progress:  18%|█▊        | 240809/1306122 [00:04<00:21, 50610.57it/s]\u001b[A\n",
      "Progress:  19%|█▉        | 245954/1306122 [00:04<00:20, 50859.20it/s]\u001b[A\n",
      "Progress:  19%|█▉        | 251057/1306122 [00:05<00:20, 50908.81it/s]\u001b[A\n",
      "Progress:  20%|█▉        | 256173/1306122 [00:05<00:20, 50982.61it/s]\u001b[A\n",
      "Progress:  20%|██        | 261272/1306122 [00:05<00:20, 50809.37it/s]\u001b[A\n",
      "Progress:  20%|██        | 266354/1306122 [00:05<00:20, 50771.72it/s]\u001b[A\n",
      "Progress:  21%|██        | 271432/1306122 [00:05<00:20, 50551.16it/s]\u001b[A\n",
      "Progress:  21%|██        | 276488/1306122 [00:05<00:20, 50489.95it/s]\u001b[A\n",
      "Progress:  22%|██▏       | 281538/1306122 [00:05<00:20, 50469.16it/s]\u001b[A\n",
      "Progress:  22%|██▏       | 286658/1306122 [00:05<00:20, 50680.70it/s]\u001b[A\n",
      "Progress:  22%|██▏       | 291727/1306122 [00:05<00:20, 50097.36it/s]\u001b[A\n",
      "Progress:  23%|██▎       | 296742/1306122 [00:05<00:20, 50111.54it/s]\u001b[A\n",
      "Progress:  23%|██▎       | 301755/1306122 [00:06<00:20, 49947.58it/s]\u001b[A\n",
      "Progress:  23%|██▎       | 306751/1306122 [00:06<00:20, 49893.73it/s]\u001b[A\n",
      "Progress:  24%|██▍       | 311798/1306122 [00:06<00:19, 50065.06it/s]\u001b[A\n",
      "Progress:  24%|██▍       | 316850/1306122 [00:06<00:19, 50197.98it/s]\u001b[A\n",
      "Progress:  25%|██▍       | 321911/1306122 [00:06<00:19, 50318.86it/s]\u001b[A\n",
      "Progress:  25%|██▌       | 327033/1306122 [00:06<00:19, 50585.76it/s]\u001b[A\n",
      "Progress:  25%|██▌       | 332131/1306122 [00:06<00:19, 50702.65it/s]\u001b[A\n",
      "Progress:  26%|██▌       | 337254/1306122 [00:06<00:19, 50858.17it/s]\u001b[A\n",
      "Progress:  26%|██▌       | 342341/1306122 [00:06<00:19, 50526.31it/s]\u001b[A\n",
      "Progress:  27%|██▋       | 347395/1306122 [00:06<00:19, 50364.75it/s]\u001b[A\n",
      "Progress:  27%|██▋       | 352433/1306122 [00:07<00:18, 50207.95it/s]\u001b[A\n",
      "Progress:  27%|██▋       | 357491/1306122 [00:07<00:18, 50317.93it/s]\u001b[A\n",
      "Progress:  28%|██▊       | 362526/1306122 [00:07<00:18, 50324.83it/s]\u001b[A\n",
      "Progress:  28%|██▊       | 367577/1306122 [00:07<00:18, 50378.06it/s]\u001b[A\n",
      "Progress:  29%|██▊       | 372667/1306122 [00:07<00:18, 50530.23it/s]\u001b[A\n",
      "Progress:  29%|██▉       | 377739/1306122 [00:07<00:18, 50585.69it/s]\u001b[A\n",
      "Progress:  29%|██▉       | 382798/1306122 [00:07<00:18, 50239.14it/s]\u001b[A\n",
      "Progress:  30%|██▉       | 387823/1306122 [00:07<00:18, 50108.53it/s]\u001b[A\n",
      "Progress:  30%|███       | 392846/1306122 [00:07<00:18, 50143.21it/s]\u001b[A\n",
      "Progress:  30%|███       | 397863/1306122 [00:07<00:18, 50148.42it/s]\u001b[A\n",
      "Progress:  31%|███       | 402898/1306122 [00:08<00:17, 50208.42it/s]\u001b[A\n",
      "Progress:  31%|███       | 407920/1306122 [00:08<00:17, 49951.28it/s]\u001b[A\n",
      "Progress:  32%|███▏      | 412916/1306122 [00:08<00:18, 49405.40it/s]\u001b[A\n",
      "Progress:  32%|███▏      | 417936/1306122 [00:08<00:17, 49639.17it/s]\u001b[A\n",
      "Progress:  32%|███▏      | 422902/1306122 [00:08<00:17, 49405.95it/s]\u001b[A\n",
      "Progress:  33%|███▎      | 427844/1306122 [00:08<00:17, 49347.08it/s]\u001b[A\n",
      "Progress:  33%|███▎      | 432780/1306122 [00:08<00:17, 49346.70it/s]\u001b[A\n",
      "Progress:  34%|███▎      | 437800/1306122 [00:08<00:17, 49597.36it/s]\u001b[A\n",
      "Progress:  34%|███▍      | 442879/1306122 [00:08<00:17, 49948.86it/s]\u001b[A\n",
      "Progress:  34%|███▍      | 448091/1306122 [00:08<00:16, 50578.70it/s]\u001b[A\n",
      "Progress:  35%|███▍      | 453231/1306122 [00:09<00:16, 50821.54it/s]\u001b[A\n",
      "Progress:  35%|███▌      | 458393/1306122 [00:09<00:16, 51058.13it/s]\u001b[A\n",
      "Progress:  35%|███▌      | 463558/1306122 [00:09<00:16, 51232.68it/s]\u001b[A\n",
      "Progress:  36%|███▌      | 468732/1306122 [00:09<00:16, 51378.48it/s]\u001b[A\n",
      "Progress:  36%|███▋      | 473871/1306122 [00:09<00:16, 51281.06it/s]\u001b[A\n",
      "Progress:  37%|███▋      | 479000/1306122 [00:09<00:16, 49769.88it/s]\u001b[A\n",
      "Progress:  37%|███▋      | 483988/1306122 [00:09<00:16, 49626.81it/s]\u001b[A\n",
      "Progress:  37%|███▋      | 489011/1306122 [00:09<00:16, 49804.34it/s]\u001b[A\n",
      "Progress:  38%|███▊      | 494053/1306122 [00:09<00:16, 49985.24it/s]\u001b[A\n",
      "Progress:  38%|███▊      | 499095/1306122 [00:09<00:16, 50113.07it/s]\u001b[A\n",
      "Progress:  39%|███▊      | 504225/1306122 [00:10<00:15, 50462.51it/s]\u001b[A\n",
      "Progress:  39%|███▉      | 509355/1306122 [00:10<00:15, 50710.70it/s]\u001b[A\n",
      "Progress:  39%|███▉      | 514500/1306122 [00:10<00:15, 50926.95it/s]\u001b[A\n",
      "Progress:  40%|███▉      | 519595/1306122 [00:10<00:15, 50908.32it/s]\u001b[A\n",
      "Progress:  40%|████      | 524701/1306122 [00:10<00:15, 50950.26it/s]\u001b[A\n",
      "Progress:  41%|████      | 529797/1306122 [00:10<00:15, 50818.50it/s]\u001b[A\n",
      "Progress:  41%|████      | 534889/1306122 [00:10<00:15, 50848.20it/s]\u001b[A\n",
      "Progress:  41%|████▏     | 540000/1306122 [00:10<00:15, 50926.22it/s]\u001b[A\n",
      "Progress:  42%|████▏     | 545093/1306122 [00:10<00:14, 50808.43it/s]\u001b[A\n",
      "Progress:  42%|████▏     | 550175/1306122 [00:10<00:14, 50661.78it/s]\u001b[A\n",
      "Progress:  43%|████▎     | 555304/1306122 [00:11<00:14, 50847.45it/s]\u001b[A\n",
      "Progress:  43%|████▎     | 560453/1306122 [00:11<00:14, 51037.05it/s]\u001b[A\n",
      "Progress:  43%|████▎     | 565558/1306122 [00:11<00:14, 50802.66it/s]\u001b[A\n",
      "Progress:  44%|████▎     | 570639/1306122 [00:11<00:14, 50350.04it/s]\u001b[A\n",
      "Progress:  44%|████▍     | 575676/1306122 [00:11<00:14, 50160.78it/s]\u001b[A\n",
      "Progress:  44%|████▍     | 580694/1306122 [00:11<00:14, 49281.00it/s]\u001b[A\n",
      "Progress:  45%|████▍     | 585739/1306122 [00:11<00:14, 49625.57it/s]\u001b[A\n",
      "Progress:  45%|████▌     | 590757/1306122 [00:11<00:14, 49788.61it/s]\u001b[A\n",
      "Progress:  46%|████▌     | 595739/1306122 [00:11<00:14, 49683.48it/s]\u001b[A\n",
      "Progress:  46%|████▌     | 600792/1306122 [00:11<00:14, 49931.81it/s]\u001b[A\n",
      "Progress:  46%|████▋     | 605879/1306122 [00:12<00:13, 50209.38it/s]\u001b[A\n",
      "Progress:  47%|████▋     | 611028/1306122 [00:12<00:13, 50583.46it/s]\u001b[A\n",
      "Progress:  47%|████▋     | 616090/1306122 [00:12<00:13, 50592.77it/s]\u001b[A\n",
      "Progress:  48%|████▊     | 621151/1306122 [00:12<00:13, 50558.18it/s]\u001b[A\n",
      "Progress:  48%|████▊     | 626230/1306122 [00:12<00:13, 50627.27it/s]\u001b[A\n",
      "Progress:  48%|████▊     | 631324/1306122 [00:12<00:13, 50720.01it/s]\u001b[A\n",
      "Progress:  49%|████▊     | 636435/1306122 [00:12<00:13, 50835.17it/s]\u001b[A\n",
      "Progress:  49%|████▉     | 641561/1306122 [00:12<00:13, 50961.64it/s]\u001b[A\n",
      "Progress:  50%|████▉     | 646658/1306122 [00:12<00:12, 50939.42it/s]\u001b[A\n",
      "Progress:  50%|████▉     | 651753/1306122 [00:12<00:12, 50926.00it/s]\u001b[A\n",
      "Progress:  50%|█████     | 656903/1306122 [00:13<00:12, 51093.93it/s]\u001b[A\n",
      "Progress:  51%|█████     | 662013/1306122 [00:13<00:12, 51029.53it/s]\u001b[A\n",
      "Progress:  51%|█████     | 667117/1306122 [00:13<00:12, 50306.14it/s]\u001b[A\n",
      "Progress:  51%|█████▏    | 672206/1306122 [00:13<00:12, 50479.11it/s]\u001b[A\n",
      "Progress:  52%|█████▏    | 677345/1306122 [00:13<00:12, 50748.01it/s]\u001b[A\n",
      "Progress:  52%|█████▏    | 682422/1306122 [00:13<00:12, 50603.16it/s]\u001b[A\n",
      "Progress:  53%|█████▎    | 687484/1306122 [00:13<00:12, 50402.06it/s]\u001b[A\n",
      "Progress:  53%|█████▎    | 692526/1306122 [00:13<00:12, 49411.80it/s]\u001b[A\n",
      "Progress:  53%|█████▎    | 697473/1306122 [00:13<00:12, 49120.97it/s]\u001b[A\n",
      "Progress:  54%|█████▍    | 702567/1306122 [00:13<00:12, 49651.62it/s]\u001b[A\n",
      "Progress:  54%|█████▍    | 707639/1306122 [00:14<00:11, 49966.72it/s]\u001b[A\n",
      "Progress:  55%|█████▍    | 712713/1306122 [00:14<00:11, 50194.25it/s]\u001b[A\n",
      "Progress:  55%|█████▍    | 717744/1306122 [00:14<00:11, 50226.03it/s]\u001b[A\n",
      "Progress:  55%|█████▌    | 722769/1306122 [00:14<00:11, 50130.82it/s]\u001b[A\n",
      "Progress:  56%|█████▌    | 727886/1306122 [00:14<00:11, 50436.71it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  56%|█████▌    | 732960/1306122 [00:14<00:11, 50526.00it/s]\u001b[A\n",
      "Progress:  57%|█████▋    | 738014/1306122 [00:14<00:11, 50384.75it/s]\u001b[A\n",
      "Progress:  57%|█████▋    | 743104/1306122 [00:14<00:11, 50537.02it/s]\u001b[A\n",
      "Progress:  57%|█████▋    | 748252/1306122 [00:14<00:10, 50815.75it/s]\u001b[A\n",
      "Progress:  58%|█████▊    | 753335/1306122 [00:14<00:10, 50800.82it/s]\u001b[A\n",
      "Progress:  58%|█████▊    | 758428/1306122 [00:15<00:10, 50838.72it/s]\u001b[A\n",
      "Progress:  58%|█████▊    | 763513/1306122 [00:15<00:10, 50628.03it/s]\u001b[A\n",
      "Progress:  59%|█████▉    | 768634/1306122 [00:15<00:10, 50800.02it/s]\u001b[A\n",
      "Progress:  59%|█████▉    | 773799/1306122 [00:15<00:10, 51050.16it/s]\u001b[A\n",
      "Progress:  60%|█████▉    | 778989/1306122 [00:15<00:10, 51299.29it/s]\u001b[A\n",
      "Progress:  60%|██████    | 784120/1306122 [00:15<00:10, 51137.80it/s]\u001b[A\n",
      "Progress:  60%|██████    | 789235/1306122 [00:15<00:10, 50865.78it/s]\u001b[A\n",
      "Progress:  61%|██████    | 794323/1306122 [00:15<00:10, 50725.58it/s]\u001b[A\n",
      "Progress:  61%|██████    | 799397/1306122 [00:15<00:10, 50575.09it/s]\u001b[A\n",
      "Progress:  62%|██████▏   | 804456/1306122 [00:15<00:09, 50541.44it/s]\u001b[A\n",
      "Progress:  62%|██████▏   | 809599/1306122 [00:16<00:09, 50804.28it/s]\u001b[A\n",
      "Progress:  62%|██████▏   | 814706/1306122 [00:16<00:09, 50883.00it/s]\u001b[A\n",
      "Progress:  63%|██████▎   | 819851/1306122 [00:16<00:09, 51048.64it/s]\u001b[A\n",
      "Progress:  63%|██████▎   | 824974/1306122 [00:16<00:09, 51101.95it/s]\u001b[A\n",
      "Progress:  64%|██████▎   | 830164/1306122 [00:16<00:09, 51337.88it/s]\u001b[A\n",
      "Progress:  64%|██████▍   | 835299/1306122 [00:16<00:09, 51282.12it/s]\u001b[A\n",
      "Progress:  64%|██████▍   | 840428/1306122 [00:16<00:09, 50685.23it/s]\u001b[A\n",
      "Progress:  65%|██████▍   | 845499/1306122 [00:16<00:09, 50419.19it/s]\u001b[A\n",
      "Progress:  65%|██████▌   | 850543/1306122 [00:16<00:09, 50313.24it/s]\u001b[A\n",
      "Progress:  66%|██████▌   | 855576/1306122 [00:17<00:08, 50239.17it/s]\u001b[A\n",
      "Progress:  66%|██████▌   | 860645/1306122 [00:17<00:08, 50372.98it/s]\u001b[A\n",
      "Progress:  66%|██████▋   | 865699/1306122 [00:17<00:08, 50422.45it/s]\u001b[A\n",
      "Progress:  67%|██████▋   | 870814/1306122 [00:17<00:08, 50638.51it/s]\u001b[A\n",
      "Progress:  67%|██████▋   | 875879/1306122 [00:17<00:08, 50553.43it/s]\u001b[A\n",
      "Progress:  67%|██████▋   | 880935/1306122 [00:17<00:08, 50458.41it/s]\u001b[A\n",
      "Progress:  68%|██████▊   | 885982/1306122 [00:17<00:08, 50303.51it/s]\u001b[A\n",
      "Progress:  68%|██████▊   | 891041/1306122 [00:17<00:08, 50386.44it/s]\u001b[A\n",
      "Progress:  69%|██████▊   | 896080/1306122 [00:17<00:08, 50349.89it/s]\u001b[A\n",
      "Progress:  69%|██████▉   | 901159/1306122 [00:17<00:08, 50479.91it/s]\u001b[A\n",
      "Progress:  69%|██████▉   | 906208/1306122 [00:18<00:07, 50397.46it/s]\u001b[A\n",
      "Progress:  70%|██████▉   | 911248/1306122 [00:18<00:07, 49701.15it/s]\u001b[A\n",
      "Progress:  70%|███████   | 916266/1306122 [00:18<00:07, 49840.84it/s]\u001b[A\n",
      "Progress:  71%|███████   | 921286/1306122 [00:18<00:07, 49947.18it/s]\u001b[A\n",
      "Progress:  71%|███████   | 926282/1306122 [00:18<00:07, 49912.42it/s]\u001b[A\n",
      "Progress:  71%|███████▏  | 931275/1306122 [00:18<00:07, 49749.36it/s]\u001b[A\n",
      "Progress:  72%|███████▏  | 936330/1306122 [00:18<00:07, 49985.16it/s]\u001b[A\n",
      "Progress:  72%|███████▏  | 941347/1306122 [00:18<00:07, 50039.48it/s]\u001b[A\n",
      "Progress:  72%|███████▏  | 946352/1306122 [00:18<00:07, 49923.01it/s]\u001b[A\n",
      "Progress:  73%|███████▎  | 951345/1306122 [00:18<00:07, 49863.29it/s]\u001b[A\n",
      "Progress:  73%|███████▎  | 956332/1306122 [00:19<00:07, 49188.61it/s]\u001b[A\n",
      "Progress:  74%|███████▎  | 961254/1306122 [00:19<00:07, 49097.01it/s]\u001b[A\n",
      "Progress:  74%|███████▍  | 966265/1306122 [00:19<00:06, 49393.85it/s]\u001b[A\n",
      "Progress:  74%|███████▍  | 971417/1306122 [00:19<00:06, 50011.97it/s]\u001b[A\n",
      "Progress:  75%|███████▍  | 976503/1306122 [00:19<00:06, 50263.38it/s]\u001b[A\n",
      "Progress:  75%|███████▌  | 981679/1306122 [00:19<00:06, 50701.79it/s]\u001b[A\n",
      "Progress:  76%|███████▌  | 986862/1306122 [00:19<00:06, 51033.85it/s]\u001b[A\n",
      "Progress:  76%|███████▌  | 991968/1306122 [00:19<00:06, 50961.47it/s]\u001b[A\n",
      "Progress:  76%|███████▋  | 997090/1306122 [00:19<00:06, 51038.15it/s]\u001b[A\n",
      "Progress:  77%|███████▋  | 1002233/1306122 [00:19<00:05, 51154.66it/s]\u001b[A\n",
      "Progress:  77%|███████▋  | 1007371/1306122 [00:20<00:05, 51220.20it/s]\u001b[A\n",
      "Progress:  78%|███████▊  | 1012532/1306122 [00:20<00:05, 51333.94it/s]\u001b[A\n",
      "Progress:  78%|███████▊  | 1017666/1306122 [00:20<00:05, 51304.33it/s]\u001b[A\n",
      "Progress:  78%|███████▊  | 1022797/1306122 [00:20<00:05, 51212.28it/s]\u001b[A\n",
      "Progress:  79%|███████▊  | 1027919/1306122 [00:20<00:05, 51038.89it/s]\u001b[A\n",
      "Progress:  79%|███████▉  | 1033024/1306122 [00:20<00:05, 51029.50it/s]\u001b[A\n",
      "Progress:  79%|███████▉  | 1038128/1306122 [00:20<00:05, 50539.31it/s]\u001b[A\n",
      "Progress:  80%|███████▉  | 1043184/1306122 [00:20<00:05, 50224.68it/s]\u001b[A\n",
      "Progress:  80%|████████  | 1048265/1306122 [00:20<00:05, 50392.58it/s]\u001b[A\n",
      "Progress:  81%|████████  | 1053306/1306122 [00:20<00:05, 50383.66it/s]\u001b[A\n",
      "Progress:  81%|████████  | 1058346/1306122 [00:21<00:04, 50333.43it/s]\u001b[A\n",
      "Progress:  81%|████████▏ | 1063471/1306122 [00:21<00:04, 50604.06it/s]\u001b[A\n",
      "Progress:  82%|████████▏ | 1068629/1306122 [00:21<00:04, 50891.23it/s]\u001b[A\n",
      "Progress:  82%|████████▏ | 1073720/1306122 [00:21<00:04, 50504.96it/s]\u001b[A\n",
      "Progress:  83%|████████▎ | 1078772/1306122 [00:21<00:04, 49922.16it/s]\u001b[A\n",
      "Progress:  83%|████████▎ | 1083767/1306122 [00:21<00:04, 49339.11it/s]\u001b[A\n",
      "Progress:  83%|████████▎ | 1088705/1306122 [00:21<00:04, 49160.32it/s]\u001b[A\n",
      "Progress:  84%|████████▎ | 1093624/1306122 [00:21<00:04, 48922.82it/s]\u001b[A\n",
      "Progress:  84%|████████▍ | 1098657/1306122 [00:21<00:04, 49336.58it/s]\u001b[A\n",
      "Progress:  84%|████████▍ | 1103593/1306122 [00:21<00:04, 49082.83it/s]\u001b[A\n",
      "Progress:  85%|████████▍ | 1108516/1306122 [00:22<00:04, 49126.17it/s]\u001b[A\n",
      "Progress:  85%|████████▌ | 1113430/1306122 [00:22<00:03, 48912.07it/s]\u001b[A\n",
      "Progress:  86%|████████▌ | 1118323/1306122 [00:22<00:03, 48783.33it/s]\u001b[A\n",
      "Progress:  86%|████████▌ | 1123242/1306122 [00:22<00:03, 48903.53it/s]\u001b[A\n",
      "Progress:  86%|████████▋ | 1128172/1306122 [00:22<00:03, 49021.08it/s]\u001b[A\n",
      "Progress:  87%|████████▋ | 1133075/1306122 [00:22<00:03, 48929.95it/s]\u001b[A\n",
      "Progress:  87%|████████▋ | 1137969/1306122 [00:22<00:03, 48895.22it/s]\u001b[A\n",
      "Progress:  88%|████████▊ | 1142869/1306122 [00:22<00:03, 48923.93it/s]\u001b[A\n",
      "Progress:  88%|████████▊ | 1147764/1306122 [00:22<00:03, 48931.13it/s]\u001b[A\n",
      "Progress:  88%|████████▊ | 1152658/1306122 [00:22<00:03, 48824.90it/s]\u001b[A\n",
      "Progress:  89%|████████▊ | 1157541/1306122 [00:23<00:03, 48752.97it/s]\u001b[A\n",
      "Progress:  89%|████████▉ | 1162443/1306122 [00:23<00:02, 48829.69it/s]\u001b[A\n",
      "Progress:  89%|████████▉ | 1167327/1306122 [00:23<00:02, 48788.56it/s]\u001b[A\n",
      "Progress:  90%|████████▉ | 1172206/1306122 [00:23<00:02, 48744.83it/s]\u001b[A\n",
      "Progress:  90%|█████████ | 1177085/1306122 [00:23<00:02, 48752.53it/s]\u001b[A\n",
      "Progress:  91%|█████████ | 1182042/1306122 [00:23<00:02, 48992.10it/s]\u001b[A\n",
      "Progress:  91%|█████████ | 1186977/1306122 [00:23<00:02, 49097.44it/s]\u001b[A\n",
      "Progress:  91%|█████████▏| 1192046/1306122 [00:23<00:02, 49563.26it/s]\u001b[A\n",
      "Progress:  92%|█████████▏| 1197100/1306122 [00:23<00:02, 49847.98it/s]\u001b[A\n",
      "Progress:  92%|█████████▏| 1202089/1306122 [00:23<00:02, 49857.07it/s]\u001b[A\n",
      "Progress:  92%|█████████▏| 1207195/1306122 [00:24<00:01, 50211.47it/s]\u001b[A\n",
      "Progress:  93%|█████████▎| 1212249/1306122 [00:24<00:01, 50308.16it/s]\u001b[A\n",
      "Progress:  93%|█████████▎| 1217284/1306122 [00:24<00:01, 50320.25it/s]\u001b[A\n",
      "Progress:  94%|█████████▎| 1222374/1306122 [00:24<00:01, 50486.03it/s]\u001b[A\n",
      "Progress:  94%|█████████▍| 1227540/1306122 [00:24<00:01, 50831.38it/s]\u001b[A\n",
      "Progress:  94%|█████████▍| 1232625/1306122 [00:24<00:01, 50829.89it/s]\u001b[A\n",
      "Progress:  95%|█████████▍| 1237709/1306122 [00:24<00:01, 50474.64it/s]\u001b[A\n",
      "Progress:  95%|█████████▌| 1242758/1306122 [00:24<00:01, 49811.17it/s]\u001b[A\n",
      "Progress:  96%|█████████▌| 1247800/1306122 [00:24<00:01, 49992.18it/s]\u001b[A\n",
      "Progress:  96%|█████████▌| 1252830/1306122 [00:24<00:01, 50083.03it/s]\u001b[A\n",
      "Progress:  96%|█████████▋| 1257840/1306122 [00:25<00:00, 50083.88it/s]\u001b[A\n",
      "Progress:  97%|█████████▋| 1262910/1306122 [00:25<00:00, 50264.73it/s]\u001b[A\n",
      "Progress:  97%|█████████▋| 1268020/1306122 [00:25<00:00, 50511.51it/s]\u001b[A\n",
      "Progress:  97%|█████████▋| 1273076/1306122 [00:25<00:00, 50523.03it/s]\u001b[A\n",
      "Progress:  98%|█████████▊| 1278174/1306122 [00:25<00:00, 50656.79it/s]\u001b[A\n",
      "Progress:  98%|█████████▊| 1283263/1306122 [00:25<00:00, 50725.60it/s]\u001b[A\n",
      "Progress:  99%|█████████▊| 1288373/1306122 [00:25<00:00, 50835.98it/s]\u001b[A\n",
      "Progress:  99%|█████████▉| 1293473/1306122 [00:25<00:00, 50882.44it/s]\u001b[A\n",
      "Progress:  99%|█████████▉| 1298620/1306122 [00:25<00:00, 51055.61it/s]\u001b[A\n",
      "Progress: 100%|█████████▉| 1303726/1306122 [00:25<00:00, 51028.07it/s]\u001b[A\n",
      "Progress: 100%|██████████| 1306122/1306122 [00:26<00:00, 50197.11it/s]\u001b[A\n",
      "Progress:   0%|          | 0/1306122 [00:00<?, ?it/s]\u001b[A\n",
      "Progress:   7%|▋         | 91662/1306122 [00:00<00:01, 916614.32it/s]\u001b[A\n",
      "Progress:  16%|█▌        | 207308/1306122 [00:00<00:01, 977427.36it/s]\u001b[A\n",
      "Progress:  25%|██▌       | 326969/1306122 [00:00<00:00, 1034260.54it/s]\u001b[A\n",
      "Progress:  34%|███▍      | 450208/1306122 [00:00<00:00, 1086668.82it/s]\u001b[A\n",
      "Progress:  44%|████▍     | 575662/1306122 [00:00<00:00, 1132115.22it/s]\u001b[A\n",
      "Progress:  54%|█████▍    | 703135/1306122 [00:00<00:00, 1171430.63it/s]\u001b[A\n",
      "Progress:  64%|██████▎   | 832412/1306122 [00:00<00:00, 1205370.19it/s]\u001b[A\n",
      "Progress:  74%|███████▎  | 962436/1306122 [00:00<00:00, 1232342.80it/s]\u001b[A\n",
      "Progress:  84%|████████▎ | 1093229/1306122 [00:00<00:00, 1254084.92it/s]\u001b[A\n",
      "Progress:  94%|█████████▎| 1223533/1306122 [00:01<00:00, 1268379.39it/s]\u001b[A\n",
      "Progress: 100%|██████████| 1306122/1306122 [00:01<00:00, 1215424.14it/s]\u001b[A\n",
      "Progress:   0%|          | 0/1306122 [00:00<?, ?it/s]\u001b[A\n",
      "Progress:   9%|▉         | 115459/1306122 [00:00<00:01, 1154582.84it/s]\u001b[A\n",
      "Progress:  20%|██        | 266399/1306122 [00:00<00:00, 1242183.04it/s]\u001b[A\n",
      "Progress:  33%|███▎      | 427299/1306122 [00:00<00:00, 1333376.08it/s]\u001b[A\n",
      "Progress:  45%|████▍     | 582429/1306122 [00:00<00:00, 1392041.01it/s]\u001b[A\n",
      "Progress:  57%|█████▋    | 739187/1306122 [00:00<00:00, 1440428.78it/s]\u001b[A\n",
      "Progress:  69%|██████▉   | 906885/1306122 [00:00<00:00, 1504075.82it/s]\u001b[A\n",
      "Progress:  82%|████████▏ | 1077542/1306122 [00:00<00:00, 1559588.48it/s]\u001b[A\n",
      "Progress:  96%|█████████▌| 1249739/1306122 [00:00<00:00, 1604991.97it/s]\u001b[A\n",
      "Progress: 100%|██████████| 1306122/1306122 [00:00<00:00, 1342401.34it/s]\u001b[A\n",
      "Progress:   0%|          | 0/1306122 [00:00<?, ?it/s]\u001b[A\n",
      "Progress:   1%|▏         | 18261/1306122 [00:00<00:07, 182605.39it/s]\u001b[A\n",
      "Progress:   3%|▎         | 41974/1306122 [00:00<00:06, 196133.90it/s]\u001b[A\n",
      "Progress:   5%|▌         | 66066/1306122 [00:00<00:05, 207717.38it/s]\u001b[A\n",
      "Progress:   7%|▋         | 90005/1306122 [00:00<00:05, 216301.04it/s]\u001b[A\n",
      "Progress:   9%|▊         | 114271/1306122 [00:00<00:05, 223583.71it/s]\u001b[A\n",
      "Progress:  11%|█         | 138563/1306122 [00:00<00:05, 229052.45it/s]\u001b[A\n",
      "Progress:  12%|█▏        | 162533/1306122 [00:00<00:04, 232145.79it/s]\u001b[A\n",
      "Progress:  14%|█▍        | 186644/1306122 [00:00<00:04, 234762.71it/s]\u001b[A\n",
      "Progress:  16%|█▌        | 210742/1306122 [00:00<00:04, 236593.35it/s]\u001b[A\n",
      "Progress:  18%|█▊        | 234902/1306122 [00:01<00:04, 238072.65it/s]\u001b[A\n",
      "Progress:  20%|█▉        | 259860/1306122 [00:01<00:04, 241411.10it/s]\u001b[A\n",
      "Progress:  22%|██▏       | 284242/1306122 [00:01<00:04, 242128.31it/s]\u001b[A\n",
      "Progress:  24%|██▎       | 308708/1306122 [00:01<00:04, 242879.26it/s]\u001b[A\n",
      "Progress:  25%|██▌       | 332851/1306122 [00:01<00:04, 242242.12it/s]\u001b[A\n",
      "Progress:  27%|██▋       | 356974/1306122 [00:01<00:03, 238805.97it/s]\u001b[A\n",
      "Progress:  29%|██▉       | 380795/1306122 [00:01<00:03, 236553.70it/s]\u001b[A\n",
      "Progress:  31%|███       | 404414/1306122 [00:01<00:03, 235459.27it/s]\u001b[A\n",
      "Progress:  33%|███▎      | 427936/1306122 [00:01<00:03, 233079.41it/s]\u001b[A\n",
      "Progress:  35%|███▍      | 451232/1306122 [00:01<00:03, 230650.17it/s]\u001b[A\n",
      "Progress:  36%|███▋      | 475602/1306122 [00:02<00:03, 234415.07it/s]\u001b[A\n",
      "Progress:  38%|███▊      | 500102/1306122 [00:02<00:03, 237492.82it/s]\u001b[A\n",
      "Progress:  40%|████      | 524849/1306122 [00:02<00:03, 240398.87it/s]\u001b[A\n",
      "Progress:  42%|████▏     | 549826/1306122 [00:02<00:03, 243135.26it/s]\u001b[A\n",
      "Progress:  44%|████▍     | 574585/1306122 [00:02<00:02, 244451.38it/s]\u001b[A\n",
      "Progress:  46%|████▌     | 599131/1306122 [00:02<00:02, 244751.94it/s]\u001b[A\n",
      "Progress:  48%|████▊     | 623619/1306122 [00:02<00:02, 243065.89it/s]\u001b[A\n",
      "Progress:  50%|████▉     | 648139/1306122 [00:02<00:02, 243698.12it/s]\u001b[A\n",
      "Progress:  51%|█████▏    | 672518/1306122 [00:02<00:02, 242894.49it/s]\u001b[A\n",
      "Progress:  53%|█████▎    | 696814/1306122 [00:02<00:02, 242169.88it/s]\u001b[A\n",
      "Progress:  55%|█████▌    | 721036/1306122 [00:03<00:02, 242038.72it/s]\u001b[A\n",
      "Progress:  57%|█████▋    | 745326/1306122 [00:03<00:02, 242294.45it/s]\u001b[A\n",
      "Progress:  59%|█████▉    | 769558/1306122 [00:03<00:02, 242131.78it/s]\u001b[A\n",
      "Progress:  61%|██████    | 793889/1306122 [00:03<00:02, 242481.87it/s]\u001b[A\n",
      "Progress:  63%|██████▎   | 818509/1306122 [00:03<00:02, 243585.19it/s]\u001b[A\n",
      "Progress:  65%|██████▍   | 842870/1306122 [00:03<00:01, 243217.66it/s]\u001b[A\n",
      "Progress:  66%|██████▋   | 867194/1306122 [00:03<00:01, 239823.12it/s]\u001b[A\n",
      "Progress:  68%|██████▊   | 891189/1306122 [00:03<00:01, 236534.31it/s]\u001b[A\n",
      "Progress:  70%|███████   | 914861/1306122 [00:03<00:01, 236034.59it/s]\u001b[A\n",
      "Progress:  72%|███████▏  | 938478/1306122 [00:03<00:01, 234796.91it/s]\u001b[A\n",
      "Progress:  74%|███████▍  | 963738/1306122 [00:04<00:01, 239868.55it/s]\u001b[A\n",
      "Progress:  76%|███████▌  | 988946/1306122 [00:04<00:01, 243400.11it/s]\u001b[A\n",
      "Progress:  78%|███████▊  | 1013323/1306122 [00:04<00:01, 238015.17it/s]\u001b[A\n",
      "Progress:  79%|███████▉  | 1037789/1306122 [00:04<00:01, 239967.11it/s]\u001b[A\n",
      "Progress:  81%|████████▏ | 1062329/1306122 [00:04<00:01, 241569.58it/s]\u001b[A\n",
      "Progress:  83%|████████▎ | 1086574/1306122 [00:04<00:00, 241830.51it/s]\u001b[A\n",
      "Progress:  85%|████████▌ | 1111101/1306122 [00:04<00:00, 242849.50it/s]\u001b[A\n",
      "Progress:  87%|████████▋ | 1135663/1306122 [00:04<00:00, 243673.45it/s]\u001b[A\n",
      "Progress:  89%|████████▉ | 1160043/1306122 [00:04<00:00, 243597.04it/s]\u001b[A\n",
      "Progress:  91%|█████████ | 1184533/1306122 [00:04<00:00, 243985.32it/s]\u001b[A\n",
      "Progress:  93%|█████████▎| 1209054/1306122 [00:05<00:00, 244349.94it/s]\u001b[A\n",
      "Progress:  94%|█████████▍| 1233643/1306122 [00:05<00:00, 244808.43it/s]\u001b[A\n",
      "Progress:  96%|█████████▋| 1258140/1306122 [00:05<00:00, 244855.55it/s]\u001b[A\n",
      "Progress:  98%|█████████▊| 1282628/1306122 [00:05<00:00, 242000.38it/s]\u001b[A\n",
      "Progress: 100%|██████████| 1306122/1306122 [00:05<00:00, 234356.39it/s]\u001b[A\n",
      "Progress:   0%|          | 0/1306122 [00:00<?, ?it/s]\u001b[A\n",
      "Progress:   0%|          | 1/1306122 [00:00<53:41:34,  6.76it/s]\u001b[A\n",
      "Progress:   0%|          | 5483/1306122 [00:00<37:25:45,  9.65it/s]\u001b[A\n",
      "Progress:   1%|          | 11677/1306122 [00:00<26:04:38, 13.79it/s]\u001b[A\n",
      "Progress:   1%|▏         | 17917/1306122 [00:00<18:10:04, 19.70it/s]\u001b[A\n",
      "Progress:   2%|▏         | 23679/1306122 [00:00<12:39:45, 28.13it/s]\u001b[A\n",
      "Progress:   2%|▏         | 29699/1306122 [00:00<8:49:26, 40.18it/s] \u001b[A\n",
      "Progress:   3%|▎         | 35903/1306122 [00:00<6:08:54, 57.39it/s]\u001b[A\n",
      "Progress:   3%|▎         | 41896/1306122 [00:00<4:17:07, 81.95it/s]\u001b[A\n",
      "Progress:   4%|▎         | 47774/1306122 [00:00<2:59:15, 117.00it/s]\u001b[A\n",
      "Progress:   4%|▍         | 53795/1306122 [00:01<2:04:58, 167.00it/s]\u001b[A\n",
      "Progress:   5%|▍         | 59699/1306122 [00:01<1:27:10, 238.28it/s]\u001b[A\n",
      "Progress:   5%|▌         | 65891/1306122 [00:01<1:00:49, 339.85it/s]\u001b[A\n",
      "Progress:   6%|▌         | 71920/1306122 [00:01<42:28, 484.32it/s]  \u001b[A\n",
      "Progress:   6%|▌         | 78001/1306122 [00:01<29:41, 689.54it/s]\u001b[A\n",
      "Progress:   6%|▋         | 84101/1306122 [00:01<20:46, 980.30it/s]\u001b[A\n",
      "Progress:   7%|▋         | 90124/1306122 [00:01<14:34, 1390.70it/s]\u001b[A\n",
      "Progress:   7%|▋         | 96198/1306122 [00:01<10:14, 1967.41it/s]\u001b[A\n",
      "Progress:   8%|▊         | 102252/1306122 [00:01<07:14, 2771.98it/s]\u001b[A\n",
      "Progress:   8%|▊         | 108292/1306122 [00:01<05:08, 3883.04it/s]\u001b[A\n",
      "Progress:   9%|▉         | 114445/1306122 [00:02<03:40, 5401.11it/s]\u001b[A\n",
      "Progress:   9%|▉         | 120605/1306122 [00:02<02:39, 7436.38it/s]\u001b[A\n",
      "Progress:  10%|▉         | 126752/1306122 [00:02<01:56, 10099.71it/s]\u001b[A\n",
      "Progress:  10%|█         | 133029/1306122 [00:02<01:26, 13497.40it/s]\u001b[A\n",
      "Progress:  11%|█         | 139303/1306122 [00:02<01:06, 17654.17it/s]\u001b[A\n",
      "Progress:  11%|█         | 145497/1306122 [00:02<00:51, 22454.69it/s]\u001b[A\n",
      "Progress:  12%|█▏        | 151676/1306122 [00:02<00:41, 27740.58it/s]\u001b[A\n",
      "Progress:  12%|█▏        | 157848/1306122 [00:02<00:34, 33214.97it/s]\u001b[A\n",
      "Progress:  13%|█▎        | 164029/1306122 [00:02<00:29, 38567.57it/s]\u001b[A\n",
      "Progress:  13%|█▎        | 170201/1306122 [00:02<00:26, 43430.49it/s]\u001b[A\n",
      "Progress:  14%|█▎        | 176396/1306122 [00:03<00:23, 47707.80it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  14%|█▍        | 182572/1306122 [00:03<00:21, 51202.43it/s]\u001b[A\n",
      "Progress:  14%|█▍        | 188766/1306122 [00:03<00:20, 54010.67it/s]\u001b[A\n",
      "Progress:  15%|█▍        | 194947/1306122 [00:03<00:19, 55997.09it/s]\u001b[A\n",
      "Progress:  15%|█▌        | 201111/1306122 [00:03<00:19, 57544.14it/s]\u001b[A\n",
      "Progress:  16%|█▌        | 207272/1306122 [00:03<00:18, 58431.21it/s]\u001b[A\n",
      "Progress:  16%|█▋        | 213403/1306122 [00:03<00:18, 59089.29it/s]\u001b[A\n",
      "Progress:  17%|█▋        | 219613/1306122 [00:03<00:18, 59959.14it/s]\u001b[A\n",
      "Progress:  17%|█▋        | 225890/1306122 [00:03<00:17, 60773.35it/s]\u001b[A\n",
      "Progress:  18%|█▊        | 232072/1306122 [00:03<00:17, 61055.31it/s]\u001b[A\n",
      "Progress:  18%|█▊        | 238251/1306122 [00:04<00:17, 61110.57it/s]\u001b[A\n",
      "Progress:  19%|█▊        | 244414/1306122 [00:04<00:17, 61010.73it/s]\u001b[A\n",
      "Progress:  19%|█▉        | 250552/1306122 [00:04<00:17, 60995.33it/s]\u001b[A\n",
      "Progress:  20%|█▉        | 256677/1306122 [00:04<00:17, 60794.35it/s]\u001b[A\n",
      "Progress:  20%|██        | 262959/1306122 [00:04<00:16, 61386.41it/s]\u001b[A\n",
      "Progress:  21%|██        | 269205/1306122 [00:04<00:16, 61704.35it/s]\u001b[A\n",
      "Progress:  21%|██        | 275504/1306122 [00:04<00:16, 62083.11it/s]\u001b[A\n",
      "Progress:  22%|██▏       | 281774/1306122 [00:04<00:16, 62264.66it/s]\u001b[A\n",
      "Progress:  22%|██▏       | 288054/1306122 [00:04<00:16, 62424.01it/s]\u001b[A\n",
      "Progress:  23%|██▎       | 294368/1306122 [00:04<00:16, 62635.20it/s]\u001b[A\n",
      "Progress:  23%|██▎       | 300635/1306122 [00:05<00:16, 62579.97it/s]\u001b[A\n",
      "Progress:  23%|██▎       | 306921/1306122 [00:05<00:15, 62662.66it/s]\u001b[A\n",
      "Progress:  24%|██▍       | 313189/1306122 [00:05<00:15, 62381.88it/s]\u001b[A\n",
      "Progress:  24%|██▍       | 319429/1306122 [00:05<00:15, 61984.72it/s]\u001b[A\n",
      "Progress:  25%|██▍       | 325697/1306122 [00:05<00:15, 62190.02it/s]\u001b[A\n",
      "Progress:  25%|██▌       | 331939/1306122 [00:05<00:15, 62258.77it/s]\u001b[A\n",
      "Progress:  26%|██▌       | 338166/1306122 [00:05<00:15, 61802.70it/s]\u001b[A\n",
      "Progress:  26%|██▋       | 344348/1306122 [00:05<00:16, 59130.58it/s]\u001b[A\n",
      "Progress:  27%|██▋       | 350287/1306122 [00:05<00:16, 57526.76it/s]\u001b[A\n",
      "Progress:  27%|██▋       | 356159/1306122 [00:05<00:16, 57877.58it/s]\u001b[A\n",
      "Progress:  28%|██▊       | 362360/1306122 [00:06<00:15, 59056.18it/s]\u001b[A\n",
      "Progress:  28%|██▊       | 368522/1306122 [00:06<00:15, 59801.54it/s]\u001b[A\n",
      "Progress:  29%|██▊       | 374742/1306122 [00:06<00:15, 60500.64it/s]\u001b[A\n",
      "Progress:  29%|██▉       | 380871/1306122 [00:06<00:15, 60733.47it/s]\u001b[A\n",
      "Progress:  30%|██▉       | 387056/1306122 [00:06<00:15, 61063.89it/s]\u001b[A\n",
      "Progress:  30%|███       | 393269/1306122 [00:06<00:14, 61377.19it/s]\u001b[A\n",
      "Progress:  31%|███       | 399493/1306122 [00:06<00:14, 61632.26it/s]\u001b[A\n",
      "Progress:  31%|███       | 405661/1306122 [00:06<00:14, 61565.70it/s]\u001b[A\n",
      "Progress:  32%|███▏      | 411898/1306122 [00:06<00:14, 61802.54it/s]\u001b[A\n",
      "Progress:  32%|███▏      | 418091/1306122 [00:06<00:14, 61840.17it/s]\u001b[A\n",
      "Progress:  32%|███▏      | 424300/1306122 [00:07<00:14, 61913.60it/s]\u001b[A\n",
      "Progress:  33%|███▎      | 430513/1306122 [00:07<00:14, 61976.92it/s]\u001b[A\n",
      "Progress:  33%|███▎      | 436809/1306122 [00:07<00:13, 62266.20it/s]\u001b[A\n",
      "Progress:  34%|███▍      | 443037/1306122 [00:07<00:13, 62193.23it/s]\u001b[A\n",
      "Progress:  34%|███▍      | 449257/1306122 [00:07<00:13, 61379.25it/s]\u001b[A\n",
      "Progress:  35%|███▍      | 455398/1306122 [00:07<00:14, 59296.97it/s]\u001b[A\n",
      "Progress:  35%|███▌      | 461345/1306122 [00:07<00:14, 57576.88it/s]\u001b[A\n",
      "Progress:  36%|███▌      | 467254/1306122 [00:07<00:14, 58019.94it/s]\u001b[A\n",
      "Progress:  36%|███▌      | 473326/1306122 [00:07<00:14, 58803.97it/s]\u001b[A\n",
      "Progress:  37%|███▋      | 479389/1306122 [00:08<00:13, 59338.03it/s]\u001b[A\n",
      "Progress:  37%|███▋      | 485427/1306122 [00:08<00:13, 59646.42it/s]\u001b[A\n",
      "Progress:  38%|███▊      | 491431/1306122 [00:08<00:13, 59763.29it/s]\u001b[A\n",
      "Progress:  38%|███▊      | 497414/1306122 [00:08<00:13, 59709.86it/s]\u001b[A\n",
      "Progress:  39%|███▊      | 503693/1306122 [00:08<00:13, 60600.96it/s]\u001b[A\n",
      "Progress:  39%|███▉      | 509760/1306122 [00:08<00:13, 60214.35it/s]\u001b[A\n",
      "Progress:  39%|███▉      | 515787/1306122 [00:08<00:13, 58923.58it/s]\u001b[A\n",
      "Progress:  40%|███▉      | 521931/1306122 [00:08<00:13, 59655.24it/s]\u001b[A\n",
      "Progress:  40%|████      | 528066/1306122 [00:08<00:12, 60152.30it/s]\u001b[A\n",
      "Progress:  41%|████      | 534336/1306122 [00:08<00:12, 60893.47it/s]\u001b[A\n",
      "Progress:  41%|████▏     | 540628/1306122 [00:09<00:12, 61487.52it/s]\u001b[A\n",
      "Progress:  42%|████▏     | 546955/1306122 [00:09<00:12, 62009.34it/s]\u001b[A\n",
      "Progress:  42%|████▏     | 553258/1306122 [00:09<00:12, 62311.98it/s]\u001b[A\n",
      "Progress:  43%|████▎     | 559552/1306122 [00:09<00:11, 62496.95it/s]\u001b[A\n",
      "Progress:  43%|████▎     | 565836/1306122 [00:09<00:11, 62598.69it/s]\u001b[A\n",
      "Progress:  44%|████▍     | 572131/1306122 [00:09<00:11, 62701.23it/s]\u001b[A\n",
      "Progress:  44%|████▍     | 578410/1306122 [00:09<00:11, 62725.12it/s]\u001b[A\n",
      "Progress:  45%|████▍     | 584684/1306122 [00:09<00:11, 62718.16it/s]\u001b[A\n",
      "Progress:  45%|████▌     | 590957/1306122 [00:09<00:11, 62666.79it/s]\u001b[A\n",
      "Progress:  46%|████▌     | 597258/1306122 [00:09<00:11, 62769.02it/s]\u001b[A\n",
      "Progress:  46%|████▌     | 603536/1306122 [00:10<00:11, 62613.90it/s]\u001b[A\n",
      "Progress:  47%|████▋     | 609798/1306122 [00:10<00:11, 61665.23it/s]\u001b[A\n",
      "Progress:  47%|████▋     | 615969/1306122 [00:10<00:11, 60226.08it/s]\u001b[A\n",
      "Progress:  48%|████▊     | 622002/1306122 [00:10<00:11, 59199.81it/s]\u001b[A\n",
      "Progress:  48%|████▊     | 627933/1306122 [00:10<00:11, 58523.99it/s]\u001b[A\n",
      "Progress:  49%|████▊     | 634132/1306122 [00:10<00:11, 59520.46it/s]\u001b[A\n",
      "Progress:  49%|████▉     | 640266/1306122 [00:10<00:11, 60054.27it/s]\u001b[A\n",
      "Progress:  49%|████▉     | 646520/1306122 [00:10<00:10, 60776.26it/s]\u001b[A\n",
      "Progress:  50%|████▉     | 652665/1306122 [00:10<00:10, 60976.71it/s]\u001b[A\n",
      "Progress:  50%|█████     | 658804/1306122 [00:10<00:10, 61097.44it/s]\u001b[A\n",
      "Progress:  51%|█████     | 664920/1306122 [00:11<00:10, 61115.73it/s]\u001b[A\n",
      "Progress:  51%|█████▏    | 671035/1306122 [00:11<00:10, 61061.67it/s]\u001b[A\n",
      "Progress:  52%|█████▏    | 677174/1306122 [00:11<00:10, 61159.03it/s]\u001b[A\n",
      "Progress:  52%|█████▏    | 683395/1306122 [00:11<00:10, 61467.90it/s]\u001b[A\n",
      "Progress:  53%|█████▎    | 689630/1306122 [00:11<00:09, 61729.13it/s]\u001b[A\n",
      "Progress:  53%|█████▎    | 695843/1306122 [00:11<00:09, 61846.79it/s]\u001b[A\n",
      "Progress:  54%|█████▍    | 702049/1306122 [00:11<00:09, 61908.10it/s]\u001b[A\n",
      "Progress:  54%|█████▍    | 708241/1306122 [00:11<00:09, 61734.38it/s]\u001b[A\n",
      "Progress:  55%|█████▍    | 714416/1306122 [00:11<00:09, 61423.48it/s]\u001b[A\n",
      "Progress:  55%|█████▌    | 720560/1306122 [00:11<00:09, 60513.07it/s]\u001b[A\n",
      "Progress:  56%|█████▌    | 726615/1306122 [00:12<00:09, 59404.91it/s]\u001b[A\n",
      "Progress:  56%|█████▌    | 732673/1306122 [00:12<00:09, 59752.39it/s]\u001b[A\n",
      "Progress:  57%|█████▋    | 738654/1306122 [00:12<00:09, 59130.50it/s]\u001b[A\n",
      "Progress:  57%|█████▋    | 744573/1306122 [00:12<00:09, 58920.19it/s]\u001b[A\n",
      "Progress:  57%|█████▋    | 750469/1306122 [00:12<00:09, 58491.60it/s]\u001b[A\n",
      "Progress:  58%|█████▊    | 756802/1306122 [00:12<00:09, 59861.67it/s]\u001b[A\n",
      "Progress:  58%|█████▊    | 763067/1306122 [00:12<00:08, 60670.13it/s]\u001b[A\n",
      "Progress:  59%|█████▉    | 769144/1306122 [00:12<00:08, 60652.24it/s]\u001b[A\n",
      "Progress:  59%|█████▉    | 775367/1306122 [00:12<00:08, 61116.26it/s]\u001b[A\n",
      "Progress:  60%|█████▉    | 781507/1306122 [00:12<00:08, 61198.64it/s]\u001b[A\n",
      "Progress:  60%|██████    | 787680/1306122 [00:13<00:08, 61354.44it/s]\u001b[A\n",
      "Progress:  61%|██████    | 793930/1306122 [00:13<00:08, 61692.04it/s]\u001b[A\n",
      "Progress:  61%|██████▏   | 800102/1306122 [00:13<00:08, 59769.42it/s]\u001b[A\n",
      "Progress:  62%|██████▏   | 806094/1306122 [00:13<00:08, 58071.84it/s]\u001b[A\n",
      "Progress:  62%|██████▏   | 811923/1306122 [00:13<00:08, 57040.45it/s]\u001b[A\n",
      "Progress:  63%|██████▎   | 817810/1306122 [00:13<00:08, 57576.92it/s]\u001b[A\n",
      "Progress:  63%|██████▎   | 823910/1306122 [00:13<00:08, 58561.10it/s]\u001b[A\n",
      "Progress:  64%|██████▎   | 829977/1306122 [00:13<00:08, 59176.58it/s]\u001b[A\n",
      "Progress:  64%|██████▍   | 835957/1306122 [00:13<00:07, 59361.45it/s]\u001b[A\n",
      "Progress:  64%|██████▍   | 841922/1306122 [00:14<00:07, 59447.66it/s]\u001b[A\n",
      "Progress:  65%|██████▍   | 847973/1306122 [00:14<00:07, 59760.96it/s]\u001b[A\n",
      "Progress:  65%|██████▌   | 854061/1306122 [00:14<00:07, 60089.84it/s]\u001b[A\n",
      "Progress:  66%|██████▌   | 860129/1306122 [00:14<00:07, 60264.07it/s]\u001b[A\n",
      "Progress:  66%|██████▋   | 866327/1306122 [00:14<00:07, 60767.23it/s]\u001b[A\n",
      "Progress:  67%|██████▋   | 872557/1306122 [00:14<00:07, 61218.63it/s]\u001b[A\n",
      "Progress:  67%|██████▋   | 878682/1306122 [00:14<00:06, 61200.40it/s]\u001b[A\n",
      "Progress:  68%|██████▊   | 884832/1306122 [00:14<00:06, 61288.33it/s]\u001b[A\n",
      "Progress:  68%|██████▊   | 891031/1306122 [00:14<00:06, 61496.96it/s]\u001b[A\n",
      "Progress:  69%|██████▊   | 897182/1306122 [00:14<00:06, 61184.70it/s]\u001b[A\n",
      "Progress:  69%|██████▉   | 903302/1306122 [00:15<00:06, 59959.63it/s]\u001b[A\n",
      "Progress:  70%|██████▉   | 909305/1306122 [00:15<00:06, 59583.53it/s]\u001b[A\n",
      "Progress:  70%|███████   | 915269/1306122 [00:15<00:06, 59073.60it/s]\u001b[A\n",
      "Progress:  71%|███████   | 921181/1306122 [00:15<00:06, 58811.44it/s]\u001b[A\n",
      "Progress:  71%|███████   | 927066/1306122 [00:15<00:06, 58156.81it/s]\u001b[A\n",
      "Progress:  71%|███████▏  | 932886/1306122 [00:15<00:06, 57865.24it/s]\u001b[A\n",
      "Progress:  72%|███████▏  | 938795/1306122 [00:15<00:06, 58224.98it/s]\u001b[A\n",
      "Progress:  72%|███████▏  | 944621/1306122 [00:15<00:06, 58038.77it/s]\u001b[A\n",
      "Progress:  73%|███████▎  | 950433/1306122 [00:15<00:06, 58062.05it/s]\u001b[A\n",
      "Progress:  73%|███████▎  | 956773/1306122 [00:15<00:05, 59565.75it/s]\u001b[A\n",
      "Progress:  74%|███████▎  | 963113/1306122 [00:16<00:05, 60664.33it/s]\u001b[A\n",
      "Progress:  74%|███████▍  | 969326/1306122 [00:16<00:05, 61094.63it/s]\u001b[A\n",
      "Progress:  75%|███████▍  | 975537/1306122 [00:16<00:05, 61393.63it/s]\u001b[A\n",
      "Progress:  75%|███████▌  | 981765/1306122 [00:16<00:05, 61655.80it/s]\u001b[A\n",
      "Progress:  76%|███████▌  | 988095/1306122 [00:16<00:05, 62138.25it/s]\u001b[A\n",
      "Progress:  76%|███████▌  | 994356/1306122 [00:16<00:05, 62278.20it/s]\u001b[A\n",
      "Progress:  77%|███████▋  | 1000697/1306122 [00:16<00:04, 62611.06it/s]\u001b[A\n",
      "Progress:  77%|███████▋  | 1006997/1306122 [00:16<00:04, 62725.11it/s]\u001b[A\n",
      "Progress:  78%|███████▊  | 1013315/1306122 [00:16<00:04, 62859.39it/s]\u001b[A\n",
      "Progress:  78%|███████▊  | 1019666/1306122 [00:16<00:04, 63051.75it/s]\u001b[A\n",
      "Progress:  79%|███████▊  | 1026038/1306122 [00:17<00:04, 63250.09it/s]\u001b[A\n",
      "Progress:  79%|███████▉  | 1032394/1306122 [00:17<00:04, 63341.01it/s]\u001b[A\n",
      "Progress:  80%|███████▉  | 1038729/1306122 [00:17<00:04, 63285.29it/s]\u001b[A\n",
      "Progress:  80%|████████  | 1045103/1306122 [00:17<00:04, 63419.14it/s]\u001b[A\n",
      "Progress:  81%|████████  | 1051470/1306122 [00:17<00:04, 63493.98it/s]\u001b[A\n",
      "Progress:  81%|████████  | 1057820/1306122 [00:17<00:03, 63462.90it/s]\u001b[A\n",
      "Progress:  81%|████████▏ | 1064167/1306122 [00:17<00:03, 63172.95it/s]\u001b[A\n",
      "Progress:  82%|████████▏ | 1070534/1306122 [00:17<00:03, 63320.95it/s]\u001b[A\n",
      "Progress:  82%|████████▏ | 1076905/1306122 [00:17<00:03, 63436.15it/s]\u001b[A\n",
      "Progress:  83%|████████▎ | 1083256/1306122 [00:17<00:03, 63458.17it/s]\u001b[A\n",
      "Progress:  83%|████████▎ | 1089603/1306122 [00:18<00:03, 63442.54it/s]\u001b[A\n",
      "Progress:  84%|████████▍ | 1095948/1306122 [00:18<00:03, 63393.79it/s]\u001b[A\n",
      "Progress:  84%|████████▍ | 1102288/1306122 [00:18<00:03, 63241.25it/s]\u001b[A\n",
      "Progress:  85%|████████▍ | 1108621/1306122 [00:18<00:03, 63266.15it/s]\u001b[A\n",
      "Progress:  85%|████████▌ | 1114948/1306122 [00:18<00:03, 63266.69it/s]\u001b[A\n",
      "Progress:  86%|████████▌ | 1121275/1306122 [00:18<00:02, 62324.69it/s]\u001b[A\n",
      "Progress:  86%|████████▋ | 1127541/1306122 [00:18<00:02, 62422.47it/s]\u001b[A\n",
      "Progress:  87%|████████▋ | 1133786/1306122 [00:18<00:02, 62384.18it/s]\u001b[A\n",
      "Progress:  87%|████████▋ | 1140082/1306122 [00:18<00:02, 62553.66it/s]\u001b[A\n",
      "Progress:  88%|████████▊ | 1146384/1306122 [00:18<00:02, 62691.73it/s]\u001b[A\n",
      "Progress:  88%|████████▊ | 1152724/1306122 [00:19<00:02, 62900.15it/s]\u001b[A\n",
      "Progress:  89%|████████▊ | 1159038/1306122 [00:19<00:02, 62971.89it/s]\u001b[A\n",
      "Progress:  89%|████████▉ | 1165379/1306122 [00:19<00:02, 63101.81it/s]\u001b[A\n",
      "Progress:  90%|████████▉ | 1171690/1306122 [00:19<00:02, 63074.73it/s]\u001b[A\n",
      "Progress:  90%|█████████ | 1178020/1306122 [00:19<00:02, 63139.73it/s]\u001b[A\n",
      "Progress:  91%|█████████ | 1184335/1306122 [00:19<00:01, 63004.95it/s]\u001b[A\n",
      "Progress:  91%|█████████ | 1190636/1306122 [00:19<00:01, 62152.99it/s]\u001b[A\n",
      "Progress:  92%|█████████▏| 1196963/1306122 [00:19<00:01, 62481.12it/s]\u001b[A\n",
      "Progress:  92%|█████████▏| 1203344/1306122 [00:19<00:01, 62872.58it/s]\u001b[A\n",
      "Progress:  93%|█████████▎| 1209634/1306122 [00:19<00:01, 62557.68it/s]\u001b[A\n",
      "Progress:  93%|█████████▎| 1215892/1306122 [00:20<00:01, 62226.09it/s]\u001b[A\n",
      "Progress:  94%|█████████▎| 1222117/1306122 [00:20<00:01, 62064.37it/s]\u001b[A\n",
      "Progress:  94%|█████████▍| 1228336/1306122 [00:20<00:01, 62099.98it/s]\u001b[A\n",
      "Progress:  95%|█████████▍| 1234547/1306122 [00:20<00:01, 62008.68it/s]\u001b[A\n",
      "Progress:  95%|█████████▍| 1240749/1306122 [00:20<00:01, 60213.46it/s]\u001b[A\n",
      "Progress:  95%|█████████▌| 1246783/1306122 [00:20<00:01, 59122.07it/s]\u001b[A\n",
      "Progress:  96%|█████████▌| 1252708/1306122 [00:20<00:00, 58021.17it/s]\u001b[A\n",
      "Progress:  96%|█████████▋| 1258524/1306122 [00:20<00:00, 57674.66it/s]\u001b[A\n",
      "Progress:  97%|█████████▋| 1264840/1306122 [00:20<00:00, 59215.21it/s]\u001b[A\n",
      "Progress:  97%|█████████▋| 1271000/1306122 [00:20<00:00, 59910.51it/s]\u001b[A\n",
      "Progress:  98%|█████████▊| 1277006/1306122 [00:21<00:00, 59730.96it/s]\u001b[A\n",
      "Progress:  98%|█████████▊| 1283079/1306122 [00:21<00:00, 60025.70it/s]\u001b[A\n",
      "Progress:  99%|█████████▊| 1289153/1306122 [00:21<00:00, 60236.31it/s]\u001b[A\n",
      "Progress:  99%|█████████▉| 1295322/1306122 [00:21<00:00, 60664.21it/s]\u001b[A\n",
      "Progress: 100%|█████████▉| 1301600/1306122 [00:21<00:00, 61282.72it/s]\u001b[A\n",
      "Progress: 100%|██████████| 1306122/1306122 [00:21<00:00, 60322.02it/s]\u001b[A\n",
      "Progress:   0%|          | 0/1306122 [00:00<?, ?it/s]\u001b[A\n",
      "Progress:   2%|▏         | 30123/1306122 [00:00<00:04, 301229.57it/s]\u001b[A\n",
      "Progress:   5%|▌         | 65784/1306122 [00:00<00:03, 315948.64it/s]\u001b[A\n",
      "Progress:   8%|▊         | 101743/1306122 [00:00<00:03, 327884.70it/s]\u001b[A\n",
      "Progress:  11%|█         | 137910/1306122 [00:00<00:03, 337338.05it/s]\u001b[A\n",
      "Progress:  13%|█▎        | 174237/1306122 [00:00<00:03, 344717.91it/s]\u001b[A\n",
      "Progress:  16%|█▌        | 210756/1306122 [00:00<00:03, 350612.10it/s]\u001b[A\n",
      "Progress:  19%|█▉        | 247617/1306122 [00:00<00:02, 355821.82it/s]\u001b[A\n",
      "Progress:  22%|██▏       | 284232/1306122 [00:00<00:02, 358857.67it/s]\u001b[A\n",
      "Progress:  24%|██▍       | 319410/1306122 [00:00<00:02, 356704.23it/s]\u001b[A\n",
      "Progress:  27%|██▋       | 353940/1306122 [00:01<00:02, 352581.52it/s]\u001b[A\n",
      "Progress:  30%|██▉       | 390766/1306122 [00:01<00:02, 357142.92it/s]\u001b[A\n",
      "Progress:  33%|███▎      | 427247/1306122 [00:01<00:02, 359405.29it/s]\u001b[A\n",
      "Progress:  36%|███▌      | 464172/1306122 [00:01<00:02, 362301.97it/s]\u001b[A\n",
      "Progress:  38%|███▊      | 500657/1306122 [00:01<00:02, 363060.67it/s]\u001b[A\n",
      "Progress:  41%|████      | 537202/1306122 [00:01<00:02, 363774.03it/s]\u001b[A\n",
      "Progress:  44%|████▍     | 573455/1306122 [00:01<00:02, 362925.67it/s]\u001b[A\n",
      "Progress:  47%|████▋     | 609947/1306122 [00:01<00:01, 363519.19it/s]\u001b[A\n",
      "Progress:  49%|████▉     | 646239/1306122 [00:01<00:01, 362341.42it/s]\u001b[A\n",
      "Progress:  52%|█████▏    | 682432/1306122 [00:01<00:01, 353291.57it/s]\u001b[A\n",
      "Progress:  55%|█████▍    | 717782/1306122 [00:02<00:01, 345504.79it/s]\u001b[A\n",
      "Progress:  58%|█████▊    | 752386/1306122 [00:02<00:01, 342517.17it/s]\u001b[A\n",
      "Progress:  60%|██████    | 786681/1306122 [00:02<00:01, 338334.58it/s]\u001b[A\n",
      "Progress:  63%|██████▎   | 822506/1306122 [00:02<00:01, 344071.45it/s]\u001b[A\n",
      "Progress:  66%|██████▌   | 858600/1306122 [00:02<00:01, 348962.03it/s]\u001b[A\n",
      "Progress:  69%|██████▊   | 894769/1306122 [00:02<00:01, 352683.73it/s]\u001b[A\n",
      "Progress:  71%|███████▏  | 931285/1306122 [00:02<00:01, 356335.77it/s]\u001b[A\n",
      "Progress:  74%|███████▍  | 967668/1306122 [00:02<00:00, 358550.51it/s]\u001b[A\n",
      "Progress:  77%|███████▋  | 1004324/1306122 [00:02<00:00, 360913.67it/s]\u001b[A\n",
      "Progress:  80%|███████▉  | 1040442/1306122 [00:02<00:00, 358478.84it/s]\u001b[A\n",
      "Progress:  82%|████████▏ | 1076313/1306122 [00:03<00:00, 350270.13it/s]\u001b[A\n",
      "Progress:  85%|████████▌ | 1111778/1306122 [00:03<00:00, 351572.54it/s]\u001b[A\n",
      "Progress:  88%|████████▊ | 1146977/1306122 [00:03<00:00, 345204.75it/s]\u001b[A\n",
      "Progress:  90%|█████████ | 1181739/1306122 [00:03<00:00, 345924.66it/s]\u001b[A\n",
      "Progress:  93%|█████████▎| 1216457/1306122 [00:03<00:00, 346298.93it/s]\u001b[A\n",
      "Progress:  96%|█████████▌| 1251115/1306122 [00:03<00:00, 345081.81it/s]\u001b[A\n",
      "Progress:  98%|█████████▊| 1285972/1306122 [00:03<00:00, 346120.77it/s]\u001b[A\n",
      "Progress: 100%|██████████| 1306122/1306122 [00:03<00:00, 341051.44it/s]\u001b[A\n",
      "Progress:   0%|          | 0/56370 [00:00<?, ?it/s]\u001b[A\n",
      "Progress: 100%|██████████| 56370/56370 [00:00<00:00, 832167.44it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   0%|          | 0/56370 [00:00<?, ?it/s]\u001b[A\n",
      "Progress: 100%|██████████| 56370/56370 [00:00<00:00, 1128175.73it/s]\u001b[A\n",
      "Progress:   0%|          | 0/56370 [00:00<?, ?it/s]\u001b[A\n",
      "Progress:  42%|████▏     | 23755/56370 [00:00<00:00, 237539.47it/s]\u001b[A\n",
      "Progress:  78%|███████▊  | 44058/56370 [00:00<00:00, 226013.06it/s]\u001b[A\n",
      "Progress: 100%|██████████| 56370/56370 [00:00<00:00, 210125.42it/s]\u001b[A\n",
      "Progress:   0%|          | 0/56370 [00:00<?, ?it/s]\u001b[A\n",
      "Progress:   9%|▉         | 5201/56370 [00:00<00:00, 51997.90it/s]\u001b[A\n",
      "Progress:  20%|█▉        | 11181/56370 [00:00<00:00, 54115.65it/s]\u001b[A\n",
      "Progress:  31%|███       | 17205/56370 [00:00<00:00, 55815.92it/s]\u001b[A\n",
      "Progress:  41%|████▏     | 23311/56370 [00:00<00:00, 57289.96it/s]\u001b[A\n",
      "Progress:  53%|█████▎    | 29623/56370 [00:00<00:00, 58922.57it/s]\u001b[A\n",
      "Progress:  64%|██████▍   | 35978/56370 [00:00<00:00, 60238.27it/s]\u001b[A\n",
      "Progress:  75%|███████▍  | 42088/56370 [00:00<00:00, 60491.76it/s]\u001b[A\n",
      "Progress:  85%|████████▌ | 47983/56370 [00:00<00:00, 60018.84it/s]\u001b[A\n",
      "Progress:  96%|█████████▌| 54104/56370 [00:00<00:00, 60370.96it/s]\u001b[A\n",
      "Progress: 100%|██████████| 56370/56370 [00:00<00:00, 59757.92it/s]\u001b[A\n",
      "Progress:   0%|          | 0/56370 [00:00<?, ?it/s]\u001b[A\n",
      "Progress:  59%|█████▉    | 33447/56370 [00:00<00:00, 334466.33it/s]\u001b[A\n",
      "Progress: 100%|██████████| 56370/56370 [00:00<00:00, 311433.60it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, features, test_features, word_index = load_and_prec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"x_train\",x_train)\n",
    "np.save(\"x_test\",x_test)\n",
    "np.save(\"y_train\",y_train)\n",
    "np.save(\"word_index.npy\",word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = 'glove.840B.300d.txt'\n",
    "    \n",
    "    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "    \n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std = -0.005838499, 0.48782197\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    \n",
    "    # Why random embedding for OOV? what if use mean?\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    \n",
    "    #embedding_matrix = np.random.normal(emb_mean, 0, (nb_words, embed_size)) # std 0\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "glove_embeddings = load_glove(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 300)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(glove_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([      0,       1,       2, ..., 1306116, 1306118, 1306119]),\n",
       "  array([      7,      11,      12, ..., 1306117, 1306120, 1306121])),\n",
       " (array([      0,       1,       3, ..., 1306118, 1306120, 1306121]),\n",
       "  array([      2,       4,       5, ..., 1306115, 1306116, 1306119])),\n",
       " (array([      2,       4,       5, ..., 1306119, 1306120, 1306121]),\n",
       "  array([      0,       1,       3, ..., 1306101, 1306103, 1306118]))]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = list(StratifiedKFold(n_splits=3, shuffle=True, random_state=1024).split(x_train, y_train))\n",
    "splits[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code inspired from: https://github.com/anandsaha/pytorch.cyclic.learning.rate/blob/master/cls.py\n",
    "class CyclicLR(object):\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "embedding_dim = 300\n",
    "embedding_path = '../save/embedding_matrix.npy'  # or False, not use pre-trained-matrix\n",
    "use_pretrained_embedding = True\n",
    "\n",
    "hidden_size = 60\n",
    "gru_len = hidden_size\n",
    "\n",
    "Routings = 4 #5\n",
    "Num_capsule = 5\n",
    "Dim_capsule = 5#16\n",
    "dropout_p = 0.25\n",
    "rate_drop_dense = 0.28\n",
    "LR = 0.001\n",
    "T_epsilon = 1e-7\n",
    "num_classes = 30\n",
    "\n",
    "\n",
    "class Embed_Layer(nn.Module):\n",
    "    def __init__(self, embedding_matrix=None, vocab_size=None, embedding_dim=300):\n",
    "        super(Embed_Layer, self).__init__()\n",
    "        self.encoder = nn.Embedding(vocab_size + 1, embedding_dim)\n",
    "        if use_pretrained_embedding:\n",
    "            self.encoder.weight.data.copy_(t.from_numpy(embedding_matrix)) \n",
    "\n",
    "    def forward(self, x, dropout_p=0.25):\n",
    "        return nn.Dropout(p=dropout_p)(self.encoder(x))\n",
    "\n",
    "\n",
    "class GRU_Layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRU_Layer, self).__init__()\n",
    "        self.gru = nn.GRU(input_size=300,\n",
    "                          hidden_size=gru_len,\n",
    "                          bidirectional=True)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for k in ih:\n",
    "            nn.init.xavier_uniform_(k)\n",
    "        for k in hh:\n",
    "            nn.init.orthogonal_(k)\n",
    "        for k in b:\n",
    "            nn.init.constant_(k, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gru(x)\n",
    "\n",
    "\n",
    "# core caps_layer with squash func\n",
    "class Caps_Layer(nn.Module):\n",
    "    def __init__(self, input_dim_capsule=gru_len * 2, num_capsule=Num_capsule, dim_capsule=Dim_capsule, \\\n",
    "                 routings=Routings, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Caps_Layer, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size \n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = self.squash\n",
    "        else:\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "        if self.share_weights:\n",
    "            self.W = nn.Parameter(\n",
    "                nn.init.xavier_normal_(t.empty(1, input_dim_capsule, self.num_capsule * self.dim_capsule)))\n",
    "        else:\n",
    "            self.W = nn.Parameter(\n",
    "                t.randn(BATCH_SIZE, input_dim_capsule, self.num_capsule * self.dim_capsule))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = t.matmul(x, self.W)\n",
    "        else:\n",
    "            print('add later')\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        input_num_capsule = x.size(1)\n",
    "        u_hat_vecs = u_hat_vecs.view((batch_size, input_num_capsule,\n",
    "                                      self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = u_hat_vecs.permute(0, 2, 1, 3)  \n",
    "        b = t.zeros_like(u_hat_vecs[:, :, :, 0])\n",
    "\n",
    "        for i in range(self.routings):\n",
    "            b = b.permute(0, 2, 1)\n",
    "            c = F.softmax(b, dim=2)\n",
    "            c = c.permute(0, 2, 1)\n",
    "            b = b.permute(0, 2, 1)\n",
    "            outputs = self.activation(t.einsum('bij,bijk->bik', (c, u_hat_vecs)))  # batch matrix multiplication\n",
    "            # outputs shape (batch_size, num_capsule, dim_capsule)\n",
    "            if i < self.routings - 1:\n",
    "                b = t.einsum('bik,bijk->bij', (outputs, u_hat_vecs))  # batch matrix multiplication\n",
    "        return outputs  # (batch_size, num_capsule, dim_capsule)\n",
    "\n",
    "    # text version of squash, slight different from original one\n",
    "    def squash(self, x, axis=-1):\n",
    "        s_squared_norm = (x ** 2).sum(axis, keepdim=True)\n",
    "        scale = t.sqrt(s_squared_norm + T_epsilon)\n",
    "        return x / scale\n",
    "    \n",
    "class Capsule_Main(nn.Module):\n",
    "    def __init__(self, embedding_matrix=None, vocab_size=None):\n",
    "        super(Capsule_Main, self).__init__()\n",
    "        self.embed_layer = Embed_Layer(embedding_matrix, vocab_size)\n",
    "        self.gru_layer = GRU_Layer()\n",
    "        self.gru_layer.init_weights()\n",
    "        self.caps_layer = Caps_Layer()\n",
    "        self.dense_layer = Dense_Layer()\n",
    "\n",
    "    def forward(self, content):\n",
    "        content1 = self.embed_layer(content)\n",
    "        content2, _ = self.gru_layer(content1)  \n",
    "        content3 = self.caps_layer(content2)\n",
    "        output = self.dense_layer(content3)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        fc_layer = 16\n",
    "        fc_layer1 = 16\n",
    "\n",
    "        self.embedding = nn.Embedding(max_features, 300)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(glove_embeddings, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        self.lstm = nn.LSTM(300, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.lstm_attention = Attention(hidden_size * 2, max_len)\n",
    "        self.gru_attention = Attention(hidden_size * 2, max_len)\n",
    "        self.bn = nn.BatchNorm1d(16, momentum=0.5)\n",
    "        self.linear = nn.Linear(hidden_size*8+3, fc_layer1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(fc_layer**2,fc_layer)\n",
    "        self.out = nn.Linear(fc_layer, 1)\n",
    "        self.lincaps = nn.Linear(Num_capsule * Dim_capsule, 1)\n",
    "        self.caps_layer = Caps_Layer()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Capsule(num_capsule=10, dim_capsule=10, routings=4, share_weights=True)(x)\n",
    "        h_embedding = self.embedding(x[0])\n",
    "        h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n",
    "        \n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        h_gru, _ = self.gru(h_lstm)\n",
    "\n",
    "        ##Capsule Layer        \n",
    "        content3 = self.caps_layer(h_gru)\n",
    "        content3 = self.dropout(content3)\n",
    "        batch_size = content3.size(0)\n",
    "        content3 = content3.view(batch_size, -1)\n",
    "        content3 = self.relu(self.lincaps(content3))\n",
    "\n",
    "        ##Attention Layer\n",
    "        h_lstm_atten = self.lstm_attention(h_lstm)\n",
    "        h_gru_atten = self.gru_attention(h_gru)\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_gru, 1)\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_gru, 1)\n",
    "        \n",
    "        f = torch.tensor(x[1], dtype=torch.float).cuda()\n",
    "\n",
    "                #[512,160]\n",
    "        conc = torch.cat((h_lstm_atten, h_gru_atten,content3, avg_pool, max_pool,f), 1)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.bn(conc)\n",
    "        conc = self.dropout(conc)\n",
    "\n",
    "        out = self.out(conc)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.dataset[index]\n",
    "        return data, target, index\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sradheya/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6 \t loss=8.5056 \t val_loss=6.6232 \t time=245.40s\n",
      "Epoch 2/6 \t loss=7.3858 \t val_loss=6.4139 \t time=254.25s\n",
      "Epoch 3/6 \t loss=6.9557 \t val_loss=6.3291 \t time=247.63s\n",
      "Epoch 4/6 \t loss=6.5649 \t val_loss=6.5784 \t time=246.74s\n",
      "Epoch 5/6 \t loss=6.1975 \t val_loss=6.5116 \t time=246.22s\n",
      "Epoch 6/6 \t loss=5.9040 \t val_loss=6.9627 \t time=245.78s\n",
      "Fold 2\n",
      "Epoch 1/6 \t loss=8.3089 \t val_loss=7.0705 \t time=250.05s\n",
      "Epoch 2/6 \t loss=7.3184 \t val_loss=6.5017 \t time=245.79s\n",
      "Epoch 3/6 \t loss=6.8889 \t val_loss=6.6818 \t time=245.70s\n",
      "Epoch 4/6 \t loss=6.5237 \t val_loss=6.8291 \t time=246.65s\n",
      "Epoch 5/6 \t loss=6.1840 \t val_loss=7.2919 \t time=245.77s\n",
      "Epoch 6/6 \t loss=5.8462 \t val_loss=6.9873 \t time=245.79s\n",
      "Fold 3\n",
      "Epoch 1/6 \t loss=8.3334 \t val_loss=6.6513 \t time=245.49s\n",
      "Epoch 2/6 \t loss=7.3470 \t val_loss=6.5372 \t time=245.66s\n",
      "Epoch 3/6 \t loss=6.8998 \t val_loss=6.6104 \t time=247.41s\n",
      "Epoch 4/6 \t loss=6.5283 \t val_loss=6.5940 \t time=248.40s\n",
      "Epoch 5/6 \t loss=6.2000 \t val_loss=6.9909 \t time=248.06s\n",
      "Epoch 6/6 \t loss=5.8504 \t val_loss=6.9318 \t time=249.94s\n",
      "All \t loss=5.8669 \t val_loss=6.9606 \t \n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "train_preds = np.zeros((len(x_train)))\n",
    "test_preds = np.zeros((len(x_test)))\n",
    "\n",
    "# always call this before training for deterministic results\n",
    "seed_everything()\n",
    "\n",
    "x_test_cuda = torch.tensor(x_test, dtype=torch.long).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=64, shuffle=False)\n",
    "\n",
    "avg_losses_f = []\n",
    "avg_val_losses_f = []\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):    \n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    features = np.array(features)\n",
    "\n",
    "    x_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.long).cuda()\n",
    "    y_train_fold = torch.tensor(y_train[train_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    kfold_X_features = features[train_idx.astype(int)]\n",
    "    kfold_X_valid_features = features[valid_idx.astype(int)]\n",
    "    x_val_fold = torch.tensor(x_train[valid_idx.astype(int)], dtype=torch.long).cuda()\n",
    "    y_val_fold = torch.tensor(y_train[valid_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    # model = BiLSTM(lstm_layer=2,hidden_dim=40,dropout=DROPOUT).cuda()\n",
    "    model = NeuralNet()\n",
    "\n",
    "    # make sure everything in the model is running on the GPU\n",
    "    model.cuda()\n",
    "\n",
    "    # define binary cross entropy loss\n",
    "    # note that the model returns logit to take advantage of the log-sum-exp trick \n",
    "    # for numerical stability in the loss\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "    step_size = 300\n",
    "    base_lr, max_lr = 0.001, 0.003   \n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=max_lr)\n",
    "\n",
    "    scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, step_size=step_size, mode='exp_range', gamma=0.99994)\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    \n",
    "    train = MyDataset(train)\n",
    "    valid = MyDataset(valid)\n",
    "\n",
    "    ##No need to shuffle the data again here. Shuffling happens when splitting for kfolds.\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=64, shuffle=False)\n",
    "\n",
    "    print(f'Fold {i + 1}')\n",
    "    for epoch in range(6):\n",
    "        # set train mode of the model. This enables operations which are only applied during training like dropout\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "\n",
    "        avg_loss = 0.  \n",
    "        for i, (x_batch, y_batch, index) in enumerate(train_loader):\n",
    "            # Forward pass: compute predicted y by passing x to the model.\n",
    "            ################################################################################################            \n",
    "            f = kfold_X_features[index]\n",
    "            y_pred = model([x_batch, f])\n",
    "            ################################################################################################\n",
    "\n",
    "            ################################################################################################\n",
    "            if scheduler:\n",
    "                scheduler.batch_step()\n",
    "            ################################################################################################\n",
    "\n",
    "\n",
    "            # Compute and print loss.\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            # Before the backward pass, use the optimizer object to zero all of the\n",
    "            # gradients for the Tensors it will update (which are the learnable weights\n",
    "            # of the model)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Calling the step function on an Optimizer makes an update to its parameters\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            \n",
    "        # set evaluation mode of the model. This disabled operations which are only applied during training like dropout\n",
    "        model.eval()\n",
    "        \n",
    "        # predict all the samples in y_val_fold batch per batch\n",
    "        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "        test_preds_fold = np.zeros((len(x_test)))\n",
    "        \n",
    "        avg_val_loss = 0.\n",
    "        for i, (x_batch, y_batch, index) in enumerate(valid_loader):\n",
    "            f = kfold_X_valid_features[index]\n",
    "            y_pred = model([x_batch, f]).detach()\n",
    "            \n",
    "            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "            valid_preds_fold[i * 64:(i+1) * 64] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "        \n",
    "        elapsed_time = time.time() - start_time \n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n",
    "            epoch + 1, 6, avg_loss, avg_val_loss, elapsed_time))\n",
    "    avg_losses_f.append(avg_loss)\n",
    "    avg_val_losses_f.append(avg_val_loss) \n",
    "    # predict all samples in the test set batch per batch\n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        f = test_features[i * 64:(i+1) * 64]\n",
    "        y_pred = model([x_batch, f]).detach()\n",
    "\n",
    "        test_preds_fold[i * 64:(i+1) * 64] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "        \n",
    "    train_preds[valid_idx] = valid_preds_fold\n",
    "    test_preds += test_preds_fold / len(splits)\n",
    "\n",
    "print('All \\t loss={:.4f} \\t val_loss={:.4f} \\t '.format(np.average(avg_losses_f),np.average(avg_val_losses_f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/41 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/41 [00:00<00:06,  5.94it/s]\u001b[A\n",
      "  5%|▍         | 2/41 [00:00<00:06,  5.97it/s]\u001b[A\n",
      "  7%|▋         | 3/41 [00:00<00:06,  6.01it/s]\u001b[A\n",
      " 10%|▉         | 4/41 [00:00<00:06,  6.06it/s]\u001b[A\n",
      " 12%|█▏        | 5/41 [00:00<00:05,  6.12it/s]\u001b[A\n",
      " 15%|█▍        | 6/41 [00:00<00:05,  6.16it/s]\u001b[A\n",
      " 17%|█▋        | 7/41 [00:01<00:05,  6.15it/s]\u001b[A\n",
      " 20%|█▉        | 8/41 [00:01<00:05,  6.01it/s]\u001b[A\n",
      " 22%|██▏       | 9/41 [00:01<00:05,  5.98it/s]\u001b[A\n",
      " 24%|██▍       | 10/41 [00:01<00:05,  6.04it/s]\u001b[A\n",
      " 27%|██▋       | 11/41 [00:01<00:04,  6.07it/s]\u001b[A\n",
      " 29%|██▉       | 12/41 [00:01<00:04,  6.12it/s]\u001b[A\n",
      " 32%|███▏      | 13/41 [00:02<00:04,  6.10it/s]\u001b[A\n",
      " 34%|███▍      | 14/41 [00:02<00:04,  6.14it/s]\u001b[A\n",
      " 37%|███▋      | 15/41 [00:02<00:04,  6.18it/s]\u001b[A\n",
      " 39%|███▉      | 16/41 [00:02<00:04,  6.18it/s]\u001b[A\n",
      " 41%|████▏     | 17/41 [00:02<00:03,  6.19it/s]\u001b[A\n",
      " 44%|████▍     | 18/41 [00:02<00:03,  6.25it/s]\u001b[A\n",
      " 46%|████▋     | 19/41 [00:03<00:03,  6.28it/s]\u001b[A\n",
      " 49%|████▉     | 20/41 [00:03<00:03,  6.31it/s]\u001b[A\n",
      " 51%|█████     | 21/41 [00:03<00:03,  6.33it/s]\u001b[A\n",
      " 54%|█████▎    | 22/41 [00:03<00:03,  6.33it/s]\u001b[A\n",
      " 56%|█████▌    | 23/41 [00:03<00:02,  6.32it/s]\u001b[A\n",
      " 59%|█████▊    | 24/41 [00:03<00:02,  6.33it/s]\u001b[A\n",
      " 61%|██████    | 25/41 [00:04<00:02,  6.34it/s]\u001b[A\n",
      " 63%|██████▎   | 26/41 [00:04<00:02,  6.36it/s]\u001b[A\n",
      " 66%|██████▌   | 27/41 [00:04<00:02,  6.37it/s]\u001b[A\n",
      " 68%|██████▊   | 28/41 [00:04<00:02,  6.39it/s]\u001b[A\n",
      " 71%|███████   | 29/41 [00:04<00:01,  6.34it/s]\u001b[A\n",
      " 73%|███████▎  | 30/41 [00:04<00:01,  6.37it/s]\u001b[A\n",
      " 76%|███████▌  | 31/41 [00:04<00:01,  6.38it/s]\u001b[A\n",
      " 78%|███████▊  | 32/41 [00:05<00:01,  6.37it/s]\u001b[A\n",
      " 80%|████████  | 33/41 [00:05<00:01,  6.38it/s]\u001b[A\n",
      " 83%|████████▎ | 34/41 [00:05<00:01,  6.39it/s]\u001b[A\n",
      " 85%|████████▌ | 35/41 [00:05<00:00,  6.40it/s]\u001b[A\n",
      " 88%|████████▊ | 36/41 [00:05<00:00,  6.41it/s]\u001b[A\n",
      " 90%|█████████ | 37/41 [00:05<00:00,  6.41it/s]\u001b[A\n",
      " 93%|█████████▎| 38/41 [00:06<00:00,  6.42it/s]\u001b[A\n",
      " 95%|█████████▌| 39/41 [00:06<00:00,  6.43it/s]\u001b[A\n",
      " 98%|█████████▊| 40/41 [00:06<00:00,  6.41it/s]\u001b[A\n",
      "100%|██████████| 41/41 [00:06<00:00,  6.31it/s]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold is 0.2700 with F1 score: 0.6683\n"
     ]
    }
   ],
   "source": [
    "def bestThresshold(y_train,train_preds):\n",
    "    tmp = [0,0,0] # idx, cur, max\n",
    "    delta = 0\n",
    "    for tmp[0] in tqdm(np.arange(0.1, 0.501, 0.01)):\n",
    "        tmp[1] = f1_score(y_train, np.array(train_preds)>tmp[0])\n",
    "        if tmp[1] > tmp[2]:\n",
    "            delta = tmp[0]\n",
    "            tmp[2] = tmp[1]\n",
    "    print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))\n",
    "    return delta\n",
    "delta = bestThresshold(y_train,train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = df_test[['qid']].copy()\n",
    "submission['prediction'] = (test_preds > delta).astype(int)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
